<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ZJU VAG</title>
    <link>https://zjuvag.github.io/</link>
    <description>Recent content on ZJU VAG</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 29 Jun 2019 18:31:12 +0800</lastBuildDate>
    
	    <atom:link href="https://zjuvag.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>SEQ2SEQ VIS</title>
      <link>https://zjuvag.github.io/post/seq2seq-vis/</link>
      <pubDate>Sat, 29 Jun 2019 18:31:12 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/post/seq2seq-vis/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文：SEQ2SEQ-VIS: A Visual Debugging Tool forSequence-to-Sequence Models&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;作者：Hendrik Strobelt; Sebastian Gehrmann; Michael Bhrisch; Adam Perer; Hanspeter Pfister; Alexander M. Rush&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-简介&#34;&gt;1.简介&lt;/h2&gt;

&lt;p&gt;本文介绍了一种针对 seq2seq 模型的可视化调试工具 Seq2Seq-Vis，从而更高效地进行分析和调试模型。&lt;/p&gt;

&lt;h2 id=&#34;2-背景&#34;&gt;2.背景&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Sequence to Sequence：RNN最重要的一个变种，也叫Encoder-Decoder模型。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder，encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义。decoder则负责根据语义向量生成指定的序列。&lt;/p&gt;

&lt;p&gt;这个模型不限制输入和输出的序列长度，应用非常广泛。主要包括：机器翻译，自然语言生成、图像描述以及文本摘要等。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attention-based Model：指的是神经网络模型增加Attention机制。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体来说，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。举例：翻译“Tom chase Jerry”中的Jerry时，“Tom”，“chase”和“Jerry”分别拥有不同的attention权重，即这三个词对翻译后的“杰瑞”有不同的影响力。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;本文模型：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/PVmHTdHpiYgMWPji.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;seq2seq 模型通过五个阶段，将源序列翻译为目标序列：（S1）Encoder：将源序列编码为潜在向量（S2）Decoder：将其解码为目标序列（S3）Attention：编码器和解码器之间注意力联系（S4）Prediction：在每个时间步骤中，预测单词概率（S5）beam Search：集束搜索。&lt;/p&gt;

&lt;p&gt;重点讲一下beam Search：&lt;/p&gt;

&lt;p&gt;Seq2seq模型在给定前缀的情况下预测所有下一个单词的概率。虽然人们可以在每个时间步骤中简单地采用最高概率词，但是这种选择可能导致错误的路径。设置Beam Size为k，永远选择所有序列中概率最大的k个。一旦所有K个光束通过生成停止标记而终止，则最终预测是具有最高分数的翻译。&lt;/p&gt;

&lt;h2 id=&#34;3-动机&#34;&gt;3.动机&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/95fC5hnHsxIz1Tvg.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;“黑箱问题”&lt;/p&gt;

&lt;p&gt;1）神经网络模型的复杂结构导致不可解释性和不确定性。&lt;/p&gt;

&lt;p&gt;2）决策过程的不透明导致无法追踪错误源头。&lt;/p&gt;

&lt;h2 id=&#34;4-本文工作&#34;&gt;4.本文工作&lt;/h2&gt;

&lt;p&gt;Seq2Seq-Vis——针对 seq2seq 模型的可视化调试工具，使用户可以可视化模型执行过程中的注意力、单词预测、搜索树、状态轨迹和近邻词列表等，从而更高效地进行分析和调试。&lt;/p&gt;

&lt;p&gt;目标：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;检查模型决策：允许用户理解、描述并具体化模型的错误。&lt;/li&gt;
&lt;li&gt;连接样本和决策：通过关联内部状态与相关的训练样本来描述模型决策。&lt;/li&gt;
&lt;li&gt;测试可选决策：可以对模型内部进行操作。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;任务：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建模型所有五个阶段的常见可视编码。&lt;/li&gt;
&lt;li&gt;通过查询大型训练样例数据库，实现最近邻搜索。&lt;/li&gt;
&lt;li&gt;为模型的不同阶段生成合理的替代决策。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;5-可视化设计&#34;&gt;5.可视化设计&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;界面概览&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/GwhJftK9xXM5n4Ri.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;翻译视图（Translation View）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/rHCadTcY5UUo265s.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;E：编码器以蓝色显示。&lt;/p&gt;

&lt;p&gt;D：解码器以黄色显示。&lt;/p&gt;

&lt;p&gt;Attention Vis：注意力通过加权的二分连接显示。为了减少视觉混乱，对注意力图进行了修剪。&lt;/p&gt;

&lt;p&gt;TopK List：每个时间步的前K个预测。每个词的概率采用条形图编码，黄色高亮显示它最终选择结果。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;邻域视图（Neighborhood View）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/WiBzNbYmktEz8mWj.png!original&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Seq2seq模型在每个阶段产生高维向量，例如编码器状态，解码器状态或上下文状态， 很难直接解释，但我们可以通过查看产生类似向量的样例来估计它们的含义。&lt;/p&gt;

&lt;p&gt;G反映的是预先计算好状态的数据集的最近邻域的状态轨迹（采用t-SNE或MDS投影）。为了便于理解较长轨迹，F视图中每一个小窗口都是G视图中的切分，重点关注每一个词。H反映的是最近邻域列表。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;交互&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/nDc3sBos1fYI23uK.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;model-focused模式：直接修改模型。&lt;/p&gt;

&lt;p&gt;a. 切换成更改attention权重模式。b.选择target中待修改权重的词。c.重复点击encoder中的词，为其赋予更大的权重。d.最后确认修改。e.模型被更改，会按照新给定的权重重新计算。&lt;/p&gt;

&lt;p&gt;language-focused模式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;更改source和target，触发比较模式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;从TopK List中手动选择最佳单词。触发前缀解码，在特定路径P上强制进行集束搜索。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://uploader.shimo.im/f/tz5PkzasdOs7BNJk.png!thumbnail&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;6-总结与未来工作&#34;&gt;6.总结与未来工作&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;针对机器翻译中最常用的 Seq2Seq 模型，提出Seq2Seq-Vis可视化调试工具，能够深入探索模型的所有阶段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;既能可视化模型的决策过程，又允许开发人员直接修改模型。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;未来工作：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;改进投影算法。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;扩展支持的序列类型，包括音频，图像和视频等。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>location2vec: a situation-aware representation for visual exploration of urban locations</title>
      <link>https://zjuvag.github.io/publication/location2vec/</link>
      <pubDate>Mon, 11 Mar 2019 17:20:04 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/location2vec/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring the Sensitivity of Choropleths under Attribute Uncertainty.</title>
      <link>https://zjuvag.github.io/publication/geouncertainty/</link>
      <pubDate>Wed, 02 Jan 2019 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/geouncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>VAUD: A visual analysis approach for exploring spatio-temporal urban data.</title>
      <link>https://zjuvag.github.io/publication/vaud/</link>
      <pubDate>Fri, 02 Nov 2018 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/vaud/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A User Study on the Capability of Three Geo-Based Features in Analyzing and Locating Trajectories.</title>
      <link>https://zjuvag.github.io/publication/user-study/</link>
      <pubDate>Mon, 29 Oct 2018 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/user-study/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Xumeng Wang presenting GraphProtector at IEEE VAST 2018</title>
      <link>https://zjuvag.github.io/talk/2018vis-xumeng/</link>
      <pubDate>Thu, 25 Oct 2018 09:36:20 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/talk/2018vis-xumeng/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dongming Han and Jiacheng Pan presenting Structure-Based Suggestive Exploration at IEEE VAST 2018</title>
      <link>https://zjuvag.github.io/talk/2018vis-han-pan/</link>
      <pubDate>Wed, 24 Oct 2018 11:43:20 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/talk/2018vis-han-pan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms.</title>
      <link>https://zjuvag.github.io/publication/graphprotector/</link>
      <pubDate>Mon, 20 Aug 2018 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/graphprotector/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structuring Mobility Transition With an Adaptive Graph Representation.</title>
      <link>https://zjuvag.github.io/publication/amtg/</link>
      <pubDate>Thu, 02 Aug 2018 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/amtg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization.</title>
      <link>https://zjuvag.github.io/publication/echarts/</link>
      <pubDate>Fri, 01 Jun 2018 08:00:00 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/echarts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relationlines: Visual Reasoning of Egocentric Relations from Heterogeneous Urban Data</title>
      <link>https://zjuvag.github.io/publication/relationlines/</link>
      <pubDate>Thu, 01 Mar 2018 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/relationlines/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data.</title>
      <link>https://zjuvag.github.io/publication/utilityaware/</link>
      <pubDate>Tue, 29 Aug 2017 15:46:02 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/utilityaware/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Volume Illustration of Muscle from Diffusion Tensor Images.</title>
      <link>https://zjuvag.github.io/publication/vimdti/</link>
      <pubDate>Mon, 29 Jun 2009 17:20:04 +0800</pubDate>
      
      <guid>https://zjuvag.github.io/publication/vimdti/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
