[{"authors":["admin"],"categories":null,"content":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009). The team works extensively with universities and research institutions at home and abroad, including Purdue University, Hong Kong University of Science and Technology, University of California, Davis, North Carolina, Mississippi State University, Bosch North American Institute, Microsoft Research Asia, National Meteorological Administration. , Ali Group and so on. The world\u0026rsquo;s first non-photorealistic 3D GPS navigation system, developed by our group\u0026rsquo;s five members at the Bosch North American Institute, has entered the global automotive market. Our group has developed a data visualization component library, DataV, together with the data product division of Ali Group. It has now been widely used within the Ali Group. The global scale 3D numerical atmospheric visual analysis system developed by our group has been well received by the National Meteorological Administration\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zjuvag.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009).","tags":null,"title":"ZJU VAG","type":"authors"},{"authors":["bingrulin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3a2ae9400bfd4148d0695ace0ec9b9d0","permalink":"https://zjuvag.github.io/authors/bingrulin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bingrulin/","section":"authors","summary":"","tags":null,"title":"Bingru Lin","type":"authors"},{"authors":["dongminghan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"09d65d06c3a9fe08d385587c9e6e87c5","permalink":"https://zjuvag.github.io/authors/dongminghan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dongminghan/","section":"authors","summary":"","tags":null,"title":"Dongming Han","type":"authors"},{"authors":["fanyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f214ff4a10bf6ecb96d09346184e3ef6","permalink":"https://zjuvag.github.io/authors/fanyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanyan/","section":"authors","summary":"","tags":null,"title":"Fan Yan","type":"authors"},{"authors":["haozhefeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e42d6a3136ba277bd69c8cb130f9e331","permalink":"https://zjuvag.github.io/authors/haozhefeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haozhefeng/","section":"authors","summary":"","tags":null,"title":"Haozhe Feng","type":"authors"},{"authors":["honghuimei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"58c4d52fdd159c66b67b8953d7b59569","permalink":"https://zjuvag.github.io/authors/honghuimei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/honghuimei/","section":"authors","summary":"","tags":null,"title":"Honghui Mei","type":"authors"},{"authors":["jiachengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4fb9f5907ebada51e66a95964ef449db","permalink":"https://zjuvag.github.io/authors/jiachengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiachengpan/","section":"authors","summary":"","tags":null,"title":"Jiacheng Pan","type":"authors"},{"authors":["jianweizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b3c918c34f1f2f1ec813f07c2d25fc85","permalink":"https://zjuvag.github.io/authors/jianweizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianweizhang/","section":"authors","summary":"","tags":null,"title":"Jianwei Zhang","type":"authors"},{"authors":["jiapinlu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1c534ed953a645c73533138708fa334e","permalink":"https://zjuvag.github.io/authors/jiapinlu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiapinlu/","section":"authors","summary":"","tags":null,"title":"Jiapin Lu","type":"authors"},{"authors":["jiehuizhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ecf94669d677925eacdd07fb6e2eecc6","permalink":"https://zjuvag.github.io/authors/jiehuizhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiehuizhou/","section":"authors","summary":"","tags":null,"title":"Jiehui Zhou","type":"authors"},{"authors":["jiewang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"81f2479fbce3bbda2fbae0889ec3bdfc","permalink":"https://zjuvag.github.io/authors/jiewang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiewang/","section":"authors","summary":"","tags":null,"title":"Jie Wang","type":"authors"},{"authors":["jinglixu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4ce8d75fde5d06b78f0badc0477bed6c","permalink":"https://zjuvag.github.io/authors/jinglixu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jinglixu/","section":"authors","summary":"","tags":null,"title":"Jingli Xu","type":"authors"},{"authors":["junhualu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a064b7e3b2d2ced15b4acef508a1049e","permalink":"https://zjuvag.github.io/authors/junhualu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junhualu/","section":"authors","summary":"","tags":null,"title":"Junhua Lu","type":"authors"},{"authors":["kejieyu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"618564ca10a1f5ed7a789b6f96efb2fa","permalink":"https://zjuvag.github.io/authors/kejieyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kejieyu/","section":"authors","summary":"","tags":null,"title":"Kejie Yu","type":"authors"},{"authors":["linhaomeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"47178a400c59f544be2df10ed1936483","permalink":"https://zjuvag.github.io/authors/linhaomeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/linhaomeng/","section":"authors","summary":"","tags":null,"title":"Linhao Meng","type":"authors"},{"authors":["lonapalawongsupaporn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6430ec5fa919d99a92b083d106ec81db","permalink":"https://zjuvag.github.io/authors/lonapalawongsupaporn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lonapalawongsupaporn/","section":"authors","summary":"","tags":null,"title":"Lonapalawong Supaporn","type":"authors"},{"authors":["minfengzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d2e2d2887bb13006210f344c3d2b4856","permalink":"https://zjuvag.github.io/authors/minfengzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minfengzhu/","section":"authors","summary":"","tags":null,"title":"Minfeng Zhu","type":"authors"},{"authors":["noptanitchotisarn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6a7eec2abe6ed13991d82a70ef34b879","permalink":"https://zjuvag.github.io/authors/noptanitchotisarn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/noptanitchotisarn/","section":"authors","summary":"","tags":null,"title":"Noptanit Chotisarn","type":"authors"},{"authors":["rushengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6d25cf561fd51b42f5cbd8309adeaa7e","permalink":"https://zjuvag.github.io/authors/rushengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rushengpan/","section":"authors","summary":"","tags":null,"title":"Rusheng Pan","type":"authors"},{"authors":["shengjiegao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"67a79c2b02345bc1e435708934246d65","permalink":"https://zjuvag.github.io/authors/shengjiegao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shengjiegao/","section":"authors","summary":"","tags":null,"title":"Shengjie Gao","type":"authors"},{"authors":["shuyuezhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a07ca3bd20c7f1c5f1254e02b72f184e","permalink":"https://zjuvag.github.io/authors/shuyuezhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuyuezhou/","section":"authors","summary":"","tags":null,"title":"Shuyue Zhou","type":"authors"},{"authors":["siweitan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f29271012bad57d28ce97df779d9648b","permalink":"https://zjuvag.github.io/authors/siweitan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/siweitan/","section":"authors","summary":"","tags":null,"title":"Siwei Tan","type":"authors"},{"authors":["tianyezhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e9e1cdffe6711fa9a43b0864dec396c6","permalink":"https://zjuvag.github.io/authors/tianyezhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyezhang/","section":"authors","summary":"","tags":null,"title":"Tianye Zhang","type":"authors"},{"authors":["weichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"611789f74777bc3ea79ff9520e093a52","permalink":"https://zjuvag.github.io/authors/weichen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weichen/","section":"authors","summary":"","tags":null,"title":"Wei Chen","type":"authors"},{"authors":["weixiaxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f5636b7d8935f736a3e99f5992265e74","permalink":"https://zjuvag.github.io/authors/weixiaxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weixiaxu/","section":"authors","summary":"","tags":null,"title":"Weixia Xu","type":"authors"},{"authors":["wenjielu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fff1a80c32ac66eee12b7369714a3555","permalink":"https://zjuvag.github.io/authors/wenjielu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenjielu/","section":"authors","summary":"","tags":null,"title":"Wenjie Lu","type":"authors"},{"authors":["xiaodongzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dbc5ba7e8a284577aa94508220ccc51a","permalink":"https://zjuvag.github.io/authors/xiaodongzhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaodongzhao/","section":"authors","summary":"","tags":null,"title":"Xiaodong Zhao","type":"authors"},{"authors":["xvmengwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"91c1e987906416f2f88af79e18a1f92b","permalink":"https://zjuvag.github.io/authors/xvmengwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xvmengwang/","section":"authors","summary":"","tags":null,"title":"Xvmeng Wang","type":"authors"},{"authors":["yankongzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5b54040d35879bf008e0ce0d58ce8582","permalink":"https://zjuvag.github.io/authors/yankongzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yankongzhang/","section":"authors","summary":"","tags":null,"title":"Yankong Zhang","type":"authors"},{"authors":["yatingwei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7f6fca97fe9dc0fb109be329aebe5385","permalink":"https://zjuvag.github.io/authors/yatingwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yatingwei/","section":"authors","summary":"","tags":null,"title":"Yating Wei","type":"authors"},{"authors":["yichaowang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"97c3335e53dd5045cf8375077f3441ff","permalink":"https://zjuvag.github.io/authors/yichaowang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yichaowang/","section":"authors","summary":"","tags":null,"title":"Yichao Wang","type":"authors"},{"authors":["yijingliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7110456b92de4eeab4224e4188560b5f","permalink":"https://zjuvag.github.io/authors/yijingliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yijingliu/","section":"authors","summary":"","tags":null,"title":"Yijing Liu","type":"authors"},{"authors":["yingxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a3d45e8ffeb1f08addba4c8c9545f41b","permalink":"https://zjuvag.github.io/authors/yingxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yingxu/","section":"authors","summary":"","tags":null,"title":"Ying Xu","type":"authors"},{"authors":["youchenggong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"88c5cbdae50149d621ac4f6d5b2ad4f6","permalink":"https://zjuvag.github.io/authors/youchenggong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/youchenggong/","section":"authors","summary":"","tags":null,"title":"Youcheng Gong","type":"authors"},{"authors":["yuanzhehu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e5db2f36fde56f28f6f63167d31fb711","permalink":"https://zjuvag.github.io/authors/yuanzhehu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuanzhehu/","section":"authors","summary":"","tags":null,"title":"Yuanzhe Hu","type":"authors"},{"authors":["yuhuigu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"525f85d17df210782f32184018c43cd8","permalink":"https://zjuvag.github.io/authors/yuhuigu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhuigu/","section":"authors","summary":"","tags":null,"title":"Yuhui Gu","type":"authors"},{"authors":["yuxuanhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"359b2cdc705132c678015f91b08c756b","permalink":"https://zjuvag.github.io/authors/yuxuanhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuxuanhou/","section":"authors","summary":"","tags":null,"title":"Yuxuan Hou","type":"authors"},{"authors":["zexianchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"72b1c74afd05b0bbbe06e0b2c86b7c17","permalink":"https://zjuvag.github.io/authors/zexianchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zexianchen/","section":"authors","summary":"","tags":null,"title":"Zexian Chen","type":"authors"},{"authors":["zhaosonghuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3733917af20bc801c57fb5dfabff0a07","permalink":"https://zjuvag.github.io/authors/zhaosonghuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaosonghuang/","section":"authors","summary":"","tags":null,"title":"Zhaosong Huang","type":"authors"},{"authors":["zhezhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"79c59e66f2a141087e56ef47c2636661","permalink":"https://zjuvag.github.io/authors/zhezhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhezhao/","section":"authors","summary":"","tags":null,"title":"Zhe Zhao","type":"authors"},{"authors":["zhiyongwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2c51f279f4b3732c7164c6424abed2bc","permalink":"https://zjuvag.github.io/authors/zhiyongwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiyongwang/","section":"authors","summary":"","tags":null,"title":"Zhiyong Wang","type":"authors"},{"authors":["Minfeng Zhu","Wei Chen","Jiazhi Xia","Yuxin Ma","Yankong Zhang","Yuetong Luo","Zhaosong Huang","Liangjun Liu"],"categories":[],"content":"","date":1552296004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552296004,"objectID":"6de916936e6e1d6592d35c0d0ceb4176","permalink":"https://zjuvag.github.io/publication/location2vec/","publishdate":"2019-03-11T17:20:04+08:00","relpermalink":"/publication/location2vec/","section":"publication","summary":"Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.","tags":["Human mobility","word embedding","urban computing","spatio-temporal data","visual exploration"],"title":"location2vec: a situation-aware representation for visual exploration of urban locations","type":"publication"},{"authors":["Zhaosong Huang","Yafeng Lu","Elizabeth Mack","Wei Chen","and Ross Maciejewski."],"categories":[],"content":"","date":1546415162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546415162,"objectID":"9f1b1bd800361c419810ea517894999f","permalink":"https://zjuvag.github.io/publication/geouncertainty/","publishdate":"2019-01-02T15:46:02+08:00","relpermalink":"/publication/geouncertainty/","section":"publication","summary":"The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.","tags":["geospatial analysis","uncertainty","visualization","choropleth"],"title":"Exploring the Sensitivity of Choropleths under Attribute Uncertainty.","type":"publication"},{"authors":["Wei Chen","Zhaosong Huang","Feiran Wu","Minfeng Zhu","Huihua Guan","and Ross Maciejewski."],"categories":[],"content":"","date":1541144762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541144762,"objectID":"816bde2fec75bdb8e2d20358198738dd","permalink":"https://zjuvag.github.io/publication/vaud/","publishdate":"2018-11-02T15:46:02+08:00","relpermalink":"/publication/vaud/","section":"publication","summary":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.","tags":["Urban data","Visual Analysis","Visual Reasoning","Heterogeneous","Spatio-temporal"],"title":"VAUD: A visual analysis approach for exploring spatio-temporal urban data.","type":"publication"},{"authors":["Xumeng Wang","Tianlong Gu","Xiwen Cai","Tianyi Lao","Wenlong Chen","Yingcai Wu","Jinhui Yu","and Wei Chen."],"categories":[],"content":"","date":1540799162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540799162,"objectID":"d65e559d084aada3b43e63f2d262dd76","permalink":"https://zjuvag.github.io/publication/user-study/","publishdate":"2018-10-29T15:46:02+08:00","relpermalink":"/publication/user-study/","section":"publication","summary":"Visual analysis is widely applied to study human mobility due to the ability in integrating contextual information multiple data sources. Analyzing trajectory data through visualization improves the efficiency and accuracy of the analysis, yet may induce exposure of the location privacy. To balance the location privacy and analysis effectiveness, this work focuses on the behaviors of different geo-based contexts in the process of trajectory interpretation. Three types of geo-based contexts are identified after surveying 94 related literatures. We further conduct experiments to investigate their capability by evaluating how they benefit the analysis, and whether they lead to the location privacy exposure. Finally, we report and discuss interesting findings, and provide guidelines to the design of privacypreserving analysis approaches for human periodic trajectories.","tags":["Periodic Trajectory","Location Privacy","Evaluation","Geo-based Context"],"title":"A User Study on the Capability of Three Geo-Based Features in Analyzing and Locating Trajectories.","type":"publication"},{"authors":[],"categories":null,"content":"","date":1540431380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540431380,"objectID":"9cd7b7d7d5fac63c93a72566cfe6a215","permalink":"https://zjuvag.github.io/talk/2018vis-xumeng/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-xumeng/","section":"talk","summary":"VAST 2018: GraphProtector: a Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","tags":["VIS2018"],"title":"Xumeng Wang presenting GraphProtector at IEEE VAST 2018","type":"talk"},{"authors":[],"categories":null,"content":"","date":1540352600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540352600,"objectID":"9fd13cd9454b4f0973f1f27c640c29c3","permalink":"https://zjuvag.github.io/talk/2018vis-han-pan/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-han-pan/","section":"talk","summary":"InfoVis 2018: Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","tags":["VIS2018"],"title":"Dongming Han and Jiacheng Pan presenting Structure-Based Suggestive Exploration at IEEE VAST 2018","type":"talk"},{"authors":["Xumeng Wang","Wei Chen","Jia-Kai Chou","Chris Bryan","Huihua Guan","Wenlong Chen","Rusheng Pan","and Kwan-Liu Ma."],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"4b6892296392d03e8f9ec9f4b6cb6c75","permalink":"https://zjuvag.github.io/publication/graphprotector/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publication/graphprotector/","section":"publication","summary":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","tags":["Graph privacy","K-Anonymity","Structural Features","Privacy Preservation"],"title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms.","type":"publication"},{"authors":["Tianlong Gu","Minfeng Zhu","Wei Chen","Zhaosong Huang","Ross Maciejewski","and Liang Chang."],"categories":[],"content":"","date":1533195962,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533195962,"objectID":"74f997771a1ca6d75fc235b2958d0447","permalink":"https://zjuvag.github.io/publication/amtg/","publishdate":"2018-08-02T15:46:02+08:00","relpermalink":"/publication/amtg/","section":"publication","summary":"Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.","tags":["Timeline","Mobility","Mobility Transition","Mobility Patterns"],"title":"Structuring Mobility Transition With an Adaptive Graph Representation.","type":"publication"},{"authors":["Deqing Li","Honghui Mei","Yi Shen","Shuang Su","Wenli Zhang","Junting Wang","Ming Zu","Wei Chen"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"6d1ce674d8525ef5534c3dcbd157c688","permalink":"https://zjuvag.github.io/publication/echarts/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/echarts/","section":"publication","summary":"While there have been a dozen of authoring systems and programming toolkits for visual design and development, users who do not have programming skills, such as data analysts or interface designers, still may feel cumbersome to efficiently implement a web-based visualization. In this paper, we present ECharts, an open-sourced, web-based, cross-platform framework that supports the rapid construction of interactive visualization. The motivation is driven by three goals: easy-to-use, rich built-in interactions, and high performance. The kernel of ECharts is a suite of declarative visual design language that customizes built-in chart types. The underlying streaming architecture, together with a high-performance graphics renderer based on HTML5 canvas, enables the high expandability and performance of ECharts. We report the design, implementation, and applications of ECharts with a diverse variety of examples. We compare the utility and performance of ECharts with C3.js, HighCharts, and Chart.js. Results of the experiments demonstrate the efficiency and scalability of our framework. Since the first release in June 2013, ECharts has iterated 63 versions, and attracted over 22,000 star counts and over 1700 related projects in the GitHub. ECharts is regarded as a leading visualization development tool in the world, and ranks the third in the GitHub visualization tab.","tags":["Visualization","Information Visualization","Visual Design","Web-based"],"title":"ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization.","type":"publication"},{"authors":["Wei Chen","Jing Xia","Xumeng Wang","Yi Wang","Jun Chen","Tianlong Gu."],"categories":[],"content":"","date":1519890362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519890362,"objectID":"50a7821e2c5a615a6a1f400bf57af1b2","permalink":"https://zjuvag.github.io/publication/relationlines/","publishdate":"2018-03-01T15:46:02+08:00","relpermalink":"/publication/relationlines/","section":"publication","summary":"The increased accessibility of urban sensor data and the popularity of social network applications is enabling the discovery of crowd mobility and personal communication patterns. However, studying the egocentric relationships of an individual (i.e., the egocentric relations) can be very challenging because available data may refer to direct contacts, such as phone calls between individuals, or indirect contacts, such as paired location presence. In this paper, we develop methods to integrate three facets extracted from heterogeneous urban data (timelines, calls and locations) through a progressive visual reasoning and inspection scheme. Our approach uses a detect-and-filter scheme, such that, prior to visual refinement and analysis, a coarse detection is performed to extract the target individual and construct the timeline of the target. It then detects spatio-temporal co-occurrences or call-based contacts to develop the egocentric network of the individual. The filtering stage is enhanced with a line-based visual reasoning interface that facilitates flexible and comprehensive investigation of egocentric relationships and connections in terms of time, space and social networks. The integrated system, RelationLines, is demonstrated using a dataset that contains taxi GPS data, cell-base mobility data, mobile calling data, microblog data and POI data of a city with millions of citizens. We examine the effectiveness and efficiency of our system by three case studies and user review.","tags":["Location-based","Egocentric Relations","Visual Reasoning","Heterogeneous Urban Data","Timeline"],"title":"Relationlines: Visual Reasoning of Egocentric Relations from Heterogeneous Urban Data","type":"publication"},{"authors":["Xumeng Wang","Jia-Kai Chou","Wei Chen","Huihua Guan","Wenlong Chen","Tianyi Lao and Kwan-Liu Ma."],"categories":[],"content":"","date":1503992762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503992762,"objectID":"623ae77fb54e58d81edff10ba4f12f96","permalink":"https://zjuvag.github.io/publication/utilityaware/","publishdate":"2017-08-29T15:46:02+08:00","relpermalink":"/publication/utilityaware/","section":"publication","summary":"Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufcient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.","tags":["Privacy Preservating Visualization","Utility Aware Anonymization","Syntactic Anonymity","Differential Privacy"],"title":"A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data.","type":"publication"},{"authors":["Wei Chen","Zhicheng Yan","Song Zhang","John Allen Crow","David S. Ebert","R. McLaughlin","K. Mullins","R. Cooper","Zi’ang Ding","Jun Liao"],"categories":[],"content":"","date":1246267204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246267204,"objectID":"db8a1fbf9ae162d1326c5c898fdd2595","permalink":"https://zjuvag.github.io/publication/vimdti/","publishdate":"2009-06-29T17:20:04+08:00","relpermalink":"/publication/vimdti/","section":"publication","summary":"Medical illustration has demonstrated its effectiveness to depict salient anatomical features while hiding the irrelevant details. Current solutions are ineffective for visualizing fibrous structures such as muscle, because typical datasets (CT or MRI) do not contain directional details. In this paper, we introduce a new muscle illustration approach that leverages diffusion tensor imaging (DTI) data and example-based texture synthesis techniques. Beginning with a volumetric diffusion tensor image, we reformulate it into a scalar field and an auxiliary guidance vector field to represent the structure and orientation of a muscle bundle. A muscle mask derived from the input diffusion tensor image is used to classify the muscle structure. The guidance vector field is further refined to remove noise and clarify structure. To simulate the internal appearance of the muscle, we propose a new two-dimensional examplebased solid texture synthesis algorithm that builds a solid texture constrained by the guidance vector field. Illustrating the constructed scalar field and solid texture efficiently highlights the global appearance of the muscle as well as the local shape and structure of the muscle fibers in an illustrative fashion. We have applied the proposed approach to five example datasets (four pig hearts and a pig leg), demonstrating plausible illustration and expressiveness.","tags":["Illustrative Visualization","Diffusion Tensor Image","Muscle","Solid Texture Synthesis"],"title":"Volume Illustration of Muscle from Diffusion Tensor Images.","type":"publication"}]