[{"authors":["admin"],"categories":null,"content":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009). The team works extensively with universities and research institutions at home and abroad, including Purdue University, Hong Kong University of Science and Technology, University of California, Davis, North Carolina, Mississippi State University, Bosch North American Institute, Microsoft Research Asia, National Meteorological Administration. , Ali Group and so on. The world\u0026rsquo;s first non-photorealistic 3D GPS navigation system, developed by our group\u0026rsquo;s five members at the Bosch North American Institute, has entered the global automotive market. Our group has developed a data visualization component library, DataV, together with the data product division of Ali Group. It has now been widely used within the Ali Group. The global scale 3D numerical atmospheric visual analysis system developed by our group has been well received by the National Meteorological Administration\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zjuvag.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009).","tags":null,"title":"ZJU VAG","type":"authors"},{"authors":["bingrulin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3a2ae9400bfd4148d0695ace0ec9b9d0","permalink":"https://zjuvag.github.io/authors/bingrulin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bingrulin/","section":"authors","summary":"","tags":null,"title":"Bingru Lin","type":"authors"},{"authors":["dongminghan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"09d65d06c3a9fe08d385587c9e6e87c5","permalink":"https://zjuvag.github.io/authors/dongminghan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dongminghan/","section":"authors","summary":"","tags":null,"title":"Dongming Han","type":"authors"},{"authors":["fanyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f214ff4a10bf6ecb96d09346184e3ef6","permalink":"https://zjuvag.github.io/authors/fanyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanyan/","section":"authors","summary":"","tags":null,"title":"Fan Yan","type":"authors"},{"authors":["haozhefeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e42d6a3136ba277bd69c8cb130f9e331","permalink":"https://zjuvag.github.io/authors/haozhefeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haozhefeng/","section":"authors","summary":"","tags":null,"title":"Haozhe Feng","type":"authors"},{"authors":["honghuimei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"58c4d52fdd159c66b67b8953d7b59569","permalink":"https://zjuvag.github.io/authors/honghuimei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/honghuimei/","section":"authors","summary":"","tags":null,"title":"Honghui Mei","type":"authors"},{"authors":["jiachengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4fb9f5907ebada51e66a95964ef449db","permalink":"https://zjuvag.github.io/authors/jiachengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiachengpan/","section":"authors","summary":"","tags":null,"title":"Jiacheng Pan","type":"authors"},{"authors":["jianweizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b3c918c34f1f2f1ec813f07c2d25fc85","permalink":"https://zjuvag.github.io/authors/jianweizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianweizhang/","section":"authors","summary":"","tags":null,"title":"Jianwei Zhang","type":"authors"},{"authors":["jiapinlu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1c534ed953a645c73533138708fa334e","permalink":"https://zjuvag.github.io/authors/jiapinlu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiapinlu/","section":"authors","summary":"","tags":null,"title":"Jiapin Lu","type":"authors"},{"authors":["jiehuizhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ecf94669d677925eacdd07fb6e2eecc6","permalink":"https://zjuvag.github.io/authors/jiehuizhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiehuizhou/","section":"authors","summary":"","tags":null,"title":"Jiehui Zhou","type":"authors"},{"authors":["jiewang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"81f2479fbce3bbda2fbae0889ec3bdfc","permalink":"https://zjuvag.github.io/authors/jiewang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiewang/","section":"authors","summary":"","tags":null,"title":"Jie Wang","type":"authors"},{"authors":["jinglixu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4ce8d75fde5d06b78f0badc0477bed6c","permalink":"https://zjuvag.github.io/authors/jinglixu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jinglixu/","section":"authors","summary":"","tags":null,"title":"Jingli Xu","type":"authors"},{"authors":["junhualu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a064b7e3b2d2ced15b4acef508a1049e","permalink":"https://zjuvag.github.io/authors/junhualu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junhualu/","section":"authors","summary":"","tags":null,"title":"Junhua Lu","type":"authors"},{"authors":["kejieyu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"618564ca10a1f5ed7a789b6f96efb2fa","permalink":"https://zjuvag.github.io/authors/kejieyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kejieyu/","section":"authors","summary":"","tags":null,"title":"Kejie Yu","type":"authors"},{"authors":["linhaomeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"47178a400c59f544be2df10ed1936483","permalink":"https://zjuvag.github.io/authors/linhaomeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/linhaomeng/","section":"authors","summary":"","tags":null,"title":"Linhao Meng","type":"authors"},{"authors":["lonapalawongsupaporn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6430ec5fa919d99a92b083d106ec81db","permalink":"https://zjuvag.github.io/authors/lonapalawongsupaporn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lonapalawongsupaporn/","section":"authors","summary":"","tags":null,"title":"Lonapalawong Supaporn","type":"authors"},{"authors":["minfengzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d2e2d2887bb13006210f344c3d2b4856","permalink":"https://zjuvag.github.io/authors/minfengzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minfengzhu/","section":"authors","summary":"","tags":null,"title":"Minfeng Zhu","type":"authors"},{"authors":["noptanitchotisarn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6a7eec2abe6ed13991d82a70ef34b879","permalink":"https://zjuvag.github.io/authors/noptanitchotisarn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/noptanitchotisarn/","section":"authors","summary":"","tags":null,"title":"Noptanit Chotisarn","type":"authors"},{"authors":["rushengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6d25cf561fd51b42f5cbd8309adeaa7e","permalink":"https://zjuvag.github.io/authors/rushengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rushengpan/","section":"authors","summary":"","tags":null,"title":"Rusheng Pan","type":"authors"},{"authors":["shengjiegao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"67a79c2b02345bc1e435708934246d65","permalink":"https://zjuvag.github.io/authors/shengjiegao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shengjiegao/","section":"authors","summary":"","tags":null,"title":"Shengjie Gao","type":"authors"},{"authors":["shuyuezhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a07ca3bd20c7f1c5f1254e02b72f184e","permalink":"https://zjuvag.github.io/authors/shuyuezhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuyuezhou/","section":"authors","summary":"","tags":null,"title":"Shuyue Zhou","type":"authors"},{"authors":["siweitan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f29271012bad57d28ce97df779d9648b","permalink":"https://zjuvag.github.io/authors/siweitan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/siweitan/","section":"authors","summary":"","tags":null,"title":"Siwei Tan","type":"authors"},{"authors":["tianyezhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e9e1cdffe6711fa9a43b0864dec396c6","permalink":"https://zjuvag.github.io/authors/tianyezhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyezhang/","section":"authors","summary":"","tags":null,"title":"Tianye Zhang","type":"authors"},{"authors":["weichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"611789f74777bc3ea79ff9520e093a52","permalink":"https://zjuvag.github.io/authors/weichen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weichen/","section":"authors","summary":"","tags":null,"title":"Wei Chen","type":"authors"},{"authors":["weixiaxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f5636b7d8935f736a3e99f5992265e74","permalink":"https://zjuvag.github.io/authors/weixiaxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weixiaxu/","section":"authors","summary":"","tags":null,"title":"Weixia Xu","type":"authors"},{"authors":["wenjielu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fff1a80c32ac66eee12b7369714a3555","permalink":"https://zjuvag.github.io/authors/wenjielu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenjielu/","section":"authors","summary":"","tags":null,"title":"Wenjie Lu","type":"authors"},{"authors":["xiaodongzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dbc5ba7e8a284577aa94508220ccc51a","permalink":"https://zjuvag.github.io/authors/xiaodongzhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaodongzhao/","section":"authors","summary":"","tags":null,"title":"Xiaodong Zhao","type":"authors"},{"authors":["xvmengwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"91c1e987906416f2f88af79e18a1f92b","permalink":"https://zjuvag.github.io/authors/xvmengwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xvmengwang/","section":"authors","summary":"","tags":null,"title":"Xvmeng Wang","type":"authors"},{"authors":["yankongzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5b54040d35879bf008e0ce0d58ce8582","permalink":"https://zjuvag.github.io/authors/yankongzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yankongzhang/","section":"authors","summary":"","tags":null,"title":"Yankong Zhang","type":"authors"},{"authors":["yatingwei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7f6fca97fe9dc0fb109be329aebe5385","permalink":"https://zjuvag.github.io/authors/yatingwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yatingwei/","section":"authors","summary":"","tags":null,"title":"Yating Wei","type":"authors"},{"authors":["yichaowang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"97c3335e53dd5045cf8375077f3441ff","permalink":"https://zjuvag.github.io/authors/yichaowang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yichaowang/","section":"authors","summary":"","tags":null,"title":"Yichao Wang","type":"authors"},{"authors":["yijingliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7110456b92de4eeab4224e4188560b5f","permalink":"https://zjuvag.github.io/authors/yijingliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yijingliu/","section":"authors","summary":"","tags":null,"title":"Yijing Liu","type":"authors"},{"authors":["yingxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a3d45e8ffeb1f08addba4c8c9545f41b","permalink":"https://zjuvag.github.io/authors/yingxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yingxu/","section":"authors","summary":"","tags":null,"title":"Ying Xu","type":"authors"},{"authors":["youchenggong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"88c5cbdae50149d621ac4f6d5b2ad4f6","permalink":"https://zjuvag.github.io/authors/youchenggong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/youchenggong/","section":"authors","summary":"","tags":null,"title":"Youcheng Gong","type":"authors"},{"authors":["yuanzhehu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e5db2f36fde56f28f6f63167d31fb711","permalink":"https://zjuvag.github.io/authors/yuanzhehu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuanzhehu/","section":"authors","summary":"","tags":null,"title":"Yuanzhe Hu","type":"authors"},{"authors":["yuhuigu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"525f85d17df210782f32184018c43cd8","permalink":"https://zjuvag.github.io/authors/yuhuigu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhuigu/","section":"authors","summary":"","tags":null,"title":"Yuhui Gu","type":"authors"},{"authors":["yuxuanhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"359b2cdc705132c678015f91b08c756b","permalink":"https://zjuvag.github.io/authors/yuxuanhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuxuanhou/","section":"authors","summary":"","tags":null,"title":"Yuxuan Hou","type":"authors"},{"authors":["zexianchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"72b1c74afd05b0bbbe06e0b2c86b7c17","permalink":"https://zjuvag.github.io/authors/zexianchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zexianchen/","section":"authors","summary":"","tags":null,"title":"Zexian Chen","type":"authors"},{"authors":["zhaosonghuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3733917af20bc801c57fb5dfabff0a07","permalink":"https://zjuvag.github.io/authors/zhaosonghuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaosonghuang/","section":"authors","summary":"","tags":null,"title":"Zhaosong Huang","type":"authors"},{"authors":["zhezhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"79c59e66f2a141087e56ef47c2636661","permalink":"https://zjuvag.github.io/authors/zhezhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhezhao/","section":"authors","summary":"","tags":null,"title":"Zhe Zhao","type":"authors"},{"authors":["zhiyongwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2c51f279f4b3732c7164c6424abed2bc","permalink":"https://zjuvag.github.io/authors/zhiyongwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiyongwang/","section":"authors","summary":"","tags":null,"title":"Zhiyong Wang","type":"authors"},{"authors":["Minfeng Zhu","Pingbo Pan","Wei Chen","and Yi Yang"],"categories":[],"content":"","date":1554191162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554191162,"objectID":"6d8f39abdf99af87212cf08d534a860c","permalink":"https://zjuvag.github.io/publication/dmgan/","publishdate":"2019-04-02T15:46:02+08:00","relpermalink":"/publication/dmgan/","section":"publication","summary":"In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.","tags":["Generative Adversarial Networks","Text-to-Image Synthesis"],"title":"DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis.","type":"publication"},{"authors":["Minfeng Zhu","Wei Chen","Jiazhi Xia","Yuxin Ma","Yankong Zhang","Yuetong Luo","Zhaosong Huang","Liangjun Liu"],"categories":[],"content":"","date":1552296004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552296004,"objectID":"6de916936e6e1d6592d35c0d0ceb4176","permalink":"https://zjuvag.github.io/publication/location2vec/","publishdate":"2019-03-11T17:20:04+08:00","relpermalink":"/publication/location2vec/","section":"publication","summary":"Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.","tags":["Human mobility","word embedding","urban computing","spatio-temporal data","visual exploration"],"title":"location2vec: a situation-aware representation for visual exploration of urban locations","type":"publication"},{"authors":["Farah Kamw","Shamal AL-Dohuki","Ye Zhao","Thomas Eynon","David Sheets","Jing Yang","Xinyue Ye","Wei Chen"],"categories":[],"content":"","date":1547596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547596800,"objectID":"053a324e0f72d98a4a158e75aa48f9dd","permalink":"https://zjuvag.github.io/publication/accessibility_modeling/","publishdate":"2019-01-16T00:00:00Z","relpermalink":"/publication/accessibility_modeling/","section":"publication","summary":"In modern cities, service providers want to identify the regions that are hard to reach from multiple fire stations, a citizen wants to meet with friends in a restaurant close to everyone, and administrators want to find whether an area far from two bus stations needs a new one. Such tasks involve studying the dynamic accessibility of the urban structures over multiple geospatial and temporal constraints, which is an important topic in geographical sciences and urban transportation. In this paper, we present a new computational model and a visualization system that help domain users to interactively study the jointly constrained accessible regions, street segments, and Points of Interest (POIs). In particular, Urban Structure Accessibility Visualization system is built upon a new Min-Max Joint Set model, where specifically designed set operations not only represent the accessible regions but also compute the minimum and maximum access times to urban structures from the joint constraints. The computation and visualization are supported by a new graph model that accommodates the real-world dynamic traffic situation and the geographical settings of urban street segments and POIs. The visualization system allows the users to conveniently construct and manage accessible regions and visually explore the urban structures inside them.","tags":["Urban accessibility","urban trajectories","visual analytics","geo-visualization"],"title":"Urban Structure Accessibility Modeling and Visualization for Joint Spatiotemporal Constraints.","type":"publication"},{"authors":["Zhaosong Huang","Yafeng Lu","Elizabeth Mack","Wei Chen","Ross Maciejewski"],"categories":[],"content":"","date":1546415162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546415162,"objectID":"9f1b1bd800361c419810ea517894999f","permalink":"https://zjuvag.github.io/publication/geouncertainty/","publishdate":"2019-01-02T15:46:02+08:00","relpermalink":"/publication/geouncertainty/","section":"publication","summary":"The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.","tags":["geospatial analysis","uncertainty","visualization","choropleth"],"title":"Exploring the Sensitivity of Choropleths under Attribute Uncertainty.","type":"publication"},{"authors":["Jie Li","Siming Chen","Wei Chen","Gennady Andrienko","Natalia Andrienko"],"categories":[],"content":"","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"5881c759f8e2540462939523dcf3cb25","permalink":"https://zjuvag.github.io/publication/semantics_cube/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/publication/semantics_cube/","section":"publication","summary":"We propose an approach to analyzing data in which texts are associated with spatial and temporal references with the aim to understand how the text semantics vary over space and time. To represent the semantics, we apply probabilistic topic modeling. After extracting a set of topics and representing the texts by vectors of topic weights, we aggregate the data into a data cube with the dimensions corresponding to the set of topics, the set of spatial locations (e.g., regions), and the time divided into suitable intervals according to the scale of the planned analysis. Each cube cell corresponds to a combination (topic, location, time interval) and contains aggregate measures characterizing the subset of the texts concerning this topic and having the spatial and temporal references within these location and interval. Based on this structure, we systematically describe the space of analysis tasks on exploring the interrelationships among the three heterogeneous information facets, semantics, space, and time. We introduce the operations of projecting and slicing the cube, which are used to decompose complex tasks into simpler subtasks. We then present a design of a visual analytics system intended to support these subtasks. To reduce the complexity of the user interface, we apply the principles of structural, visual, and operational uniformity while respecting the specific properties of each facet. The aggregated data are represented in three parallel views corresponding to the three facets and providing different complementary perspectives on the data. The views have similar look-and-feel to the extent allowed by the facet specifics. Uniform interactive operations applicable to any view support establishing links between the facets. The uniformity principle is also applied in supporting the projecting and slicing operations on the data cube. We evaluate the feasibility and utility of the approach by applying it in two analysis scenarios using geolocated social media data for studying people's reactions to social and natural events of different spatial and temporal scales.","tags":["spatiotemporal visualization","semantic visualization","data cube","interactive exploration","visual analytics"],"title":"Semantics-Space-Time Cube: A Conceptual Framework for Systematic Analysis of Texts in Space and Time.","type":"publication"},{"authors":["Wei Chen","Zhaosong Huang","Feiran Wu","Minfeng Zhu","Huihua Guan","Ross Maciejewski"],"categories":[],"content":"","date":1541144762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541144762,"objectID":"816bde2fec75bdb8e2d20358198738dd","permalink":"https://zjuvag.github.io/publication/vaud/","publishdate":"2018-11-02T15:46:02+08:00","relpermalink":"/publication/vaud/","section":"publication","summary":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.","tags":["Urban data","Visual Analysis","Visual Reasoning","Heterogeneous","Spatio-temporal"],"title":"VAUD: A visual analysis approach for exploring spatio-temporal urban data.","type":"publication"},{"authors":["Xumeng Wang","Tianlong Gu","Xiwen Cai","Tianyi Lao","Wenlong Chen","Yingcai Wu","Jinhui Yu","Wei Chen"],"categories":[],"content":"","date":1540799162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540799162,"objectID":"d65e559d084aada3b43e63f2d262dd76","permalink":"https://zjuvag.github.io/publication/user-study/","publishdate":"2018-10-29T15:46:02+08:00","relpermalink":"/publication/user-study/","section":"publication","summary":"Visual analysis is widely applied to study human mobility due to the ability in integrating contextual information multiple data sources. Analyzing trajectory data through visualization improves the efficiency and accuracy of the analysis, yet may induce exposure of the location privacy. To balance the location privacy and analysis effectiveness, this work focuses on the behaviors of different geo-based contexts in the process of trajectory interpretation. Three types of geo-based contexts are identified after surveying 94 related literatures. We further conduct experiments to investigate their capability by evaluating how they benefit the analysis, and whether they lead to the location privacy exposure. Finally, we report and discuss interesting findings, and provide guidelines to the design of privacypreserving analysis approaches for human periodic trajectories.","tags":["Periodic Trajectory","Location Privacy","Evaluation","Geo-based Context"],"title":"A User Study on the Capability of Three Geo-Based Features in Analyzing and Locating Trajectories.","type":"publication"},{"authors":[],"categories":null,"content":"","date":1540431380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540431380,"objectID":"9cd7b7d7d5fac63c93a72566cfe6a215","permalink":"https://zjuvag.github.io/talk/2018vis-xumeng/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-xumeng/","section":"talk","summary":"VAST 2018: GraphProtector: a Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","tags":["VIS2018"],"title":"Xumeng Wang presenting GraphProtector at IEEE VAST 2018","type":"talk"},{"authors":[],"categories":null,"content":"","date":1540352600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540352600,"objectID":"9fd13cd9454b4f0973f1f27c640c29c3","permalink":"https://zjuvag.github.io/talk/2018vis-han-pan/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-han-pan/","section":"talk","summary":"InfoVis 2018: Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","tags":["VIS2018"],"title":"Dongming Han and Jiacheng Pan presenting Structure-Based Suggestive Exploration at IEEE VAST 2018","type":"talk"},{"authors":["Yuxin Ma","Anthony K. H. Tung","Wei Wang","Xiang Gao","Zhigeng Pan","Wei Chen"],"categories":[],"content":"","date":1539302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539302400,"objectID":"27caa67d1a8110a432ff66cdfcaebfab","permalink":"https://zjuvag.github.io/publication/scatternet/","publishdate":"2018-10-12T00:00:00Z","relpermalink":"/publication/scatternet/","section":"publication","summary":"Similarity measuring methods are widely adopted in a broad range of visualization applications. In this work, we address the challenge of representing human perception in the visual analysis of scatterplots by introducing a novel deep-learning-based approach, ScatterNet, captures perception-driven similarities of such plots. The approach exploits deep neural networks to extract semantic features of scatterplot images for similarity calculation. We create a large labeled dataset consisting of similar and dissimilar images of scatterplots to train the deep neural network. We conduct a set of evaluations including performance experiments and a user study to demonstrate the effectiveness and efficiency of our approach. The evaluations confirm that the learned features capture the human perception of scatterplot similarity effectively. We describe two scenarios to show how ScatterNet can be applied in visual analysis applications.","tags":["Scatterplot","similarity measuring","deep learning","visualization","visual exploration"],"title":"ScatterNet: A Deep Subjective Similarity Model for Visual Analysis of Scatterplots.","type":"publication"},{"authors":["Ying Zhao","Feng Luo","Minghui Chen","Yingchao Wang","Jiazhi Xia","Fangfang Zhou","Yunhai Wang","Yi Chen","Wei Chen"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"e1fb9e23fd582b0df541c0a381c4719a","permalink":"https://zjuvag.github.io/publication/fuzzyclustering/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publication/fuzzyclustering/","section":"publication","summary":"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.","tags":["Data visualization","Task analysis","Principal component analysis","Visualization","Encoding","Clustering algorithms","Correlation"],"title":"Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters.","type":"publication"},{"authors":["Xumeng Wang","Wei Chen","Jia-Kai Chou","Chris Bryan","Huihua Guan","Wenlong Chen","Rusheng Pan","Kwan-Liu Ma"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"4b6892296392d03e8f9ec9f4b6cb6c75","permalink":"https://zjuvag.github.io/publication/graphprotector/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publication/graphprotector/","section":"publication","summary":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","tags":["Graph privacy","K-Anonymity","Structural Features","Privacy Preservation"],"title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms.","type":"publication"},{"authors":["Zhiguang Zhou","Linhao Meng","Cheng Tang","Ying Zhao","Zhiyong Guo","Miaoxin Hu","Wei Chen"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"c9b25f70681f24156e073359ce9ec89e","permalink":"https://zjuvag.github.io/publication/odflow/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publication/odflow/","section":"publication","summary":"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.","tags":["Data visualization","Clutter","Geospatial analysis","Semantics","Mobile handsets","Correlation"],"title":"Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data.","type":"publication"},{"authors":["Yingcai Wu","Xiao Xie","Jiachen Wang","Dazhen Deng","Hongye Liang","Hui Zhang","Shoubin Cheng","Wei Chen"],"categories":[],"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"3cc6d2e379803859f0b1353c0d0f6129","permalink":"https://zjuvag.github.io/publication/forvizor/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/publication/forvizor/","section":"publication","summary":"Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.","tags":["Soccer data","formation analysis","spatio-temporal visualization"],"title":"ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer.","type":"publication"},{"authors":["Wei Chen","Fangzhou Guo","Dongming Han","Jiacheng Pan","Xiaotao Nie","Jiazhi Xia","Xiaolong Zhang"],"categories":[],"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"b3e3a8eac7cdf276ca2342c59721ec74","permalink":"https://zjuvag.github.io/publication/structure-based/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/publication/structure-based/","section":"publication","summary":"When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.","tags":["Graph","Embedding","Navigation","Network","Exploration","Structure","Search"],"title":"Structure-Based Suggestive Exploration: New Approach for Effective Exploration of Large Networks","type":"publication"},{"authors":["Tianlong Gu","Minfeng Zhu","Wei Chen","Zhaosong Huang","Ross Maciejewski","Liang Chang"],"categories":[],"content":"","date":1533195962,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533195962,"objectID":"74f997771a1ca6d75fc235b2958d0447","permalink":"https://zjuvag.github.io/publication/amtg/","publishdate":"2018-08-02T15:46:02+08:00","relpermalink":"/publication/amtg/","section":"publication","summary":"Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.","tags":["Timeline","Mobility","Mobility Transition","Mobility Patterns"],"title":"Structuring Mobility Transition With an Adaptive Graph Representation.","type":"publication"},{"authors":["Weifeng Chen","Wei Chen","Hujun Bao"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"f0dfc95b9fbc25fa4b4adbaebee92d47","permalink":"https://zjuvag.github.io/publication/dichromats/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/dichromats/","section":"publication","summary":"Color vision deficiency (CVD) affects a high percentage of the population worldwide. When seeing a volume visualization result, persons with CVD may be incapable of discriminating the classification information expressed in the image if the color transfer function or the color blending used in the direct volume rendering is not appropriate. Conventional methods used to address this problem adopt advanced image recoloring techniques to enhance the rendering results frame-by-frame; unfortunately, problematic perceptual results may still be generated. This paper proposes an alternative solution that complements the image recoloring scheme by reconfiguring the components of the direct volume rendering (DVR) pipeline. Our approach optimizes the mapped colors of a transfer function to simulate CVD-friendly effect that is generated by applying the image recoloring to the results with the initial transfer function. The optimization process has a low computational complexity, and only needs to be performed once for a given transfer function. To achieve detail-preserving and perceptually natural semi-transparent effects, we introduce a new color composition mode that works in the color space of dichromats. Experimental results and a pilot study demonstrates that our approach can yield dichromats-friendly and consistent volume visualization in real-time.","tags":["Dichromacy","direct volume rendering","volume classification","image recoloring"],"title":"An Efficient Direct Volume Rendering Algorithm for Dichromats.","type":"publication"},{"authors":["Deqing Li","Honghui Mei","Yi Shen","Shuang Su","Wenli Zhang","Junting Wang","Ming Zu","Wei Chen"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"6d1ce674d8525ef5534c3dcbd157c688","permalink":"https://zjuvag.github.io/publication/echarts/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/echarts/","section":"publication","summary":"While there have been a dozen of authoring systems and programming toolkits for visual design and development, users who do not have programming skills, such as data analysts or interface designers, still may feel cumbersome to efficiently implement a web-based visualization. In this paper, we present ECharts, an open-sourced, web-based, cross-platform framework that supports the rapid construction of interactive visualization. The motivation is driven by three goals: easy-to-use, rich built-in interactions, and high performance. The kernel of ECharts is a suite of declarative visual design language that customizes built-in chart types. The underlying streaming architecture, together with a high-performance graphics renderer based on HTML5 canvas, enables the high expandability and performance of ECharts. We report the design, implementation, and applications of ECharts with a diverse variety of examples. We compare the utility and performance of ECharts with C3.js, HighCharts, and Chart.js. Results of the experiments demonstrate the efficiency and scalability of our framework. Since the first release in June 2013, ECharts has iterated 63 versions, and attracted over 22,000 star counts and over 1700 related projects in the GitHub. ECharts is regarded as a leading visualization development tool in the world, and ranks the third in the GitHub visualization tab.","tags":["Visualization","Information Visualization","Visual Design","Web-based"],"title":"ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization.","type":"publication"},{"authors":["Yunhai Wang","Wei Chen","Jian Zhang","Tingxing Dong","Guihua Shan","Xuebin Chi"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"2bea243f0920924f54c1526f6505c14a","permalink":"https://zjuvag.github.io/publication/gaussianmixture/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/gaussianmixture/","section":"publication","summary":"The multidimensional transfer function is a flexible and effective tool for exploring volume data. However, designing an appropriate transfer function is a trial-and-error process and remains a challenge. In this paper, we propose a novel volume exploration scheme that explores volumetric structures in the feature space by modeling the space using the Gaussian mixture model (GMM). Our new approach has three distinctive advantages. First, an initial feature separation can be automatically achieved through GMM estimation. Second, the calculated Gaussians can be directly mapped to a set of elliptical transfer functions (ETFs), facilitating a fast pre-integrated volume rendering process. Third, an inexperienced user can flexibly manipulate the ETFs with the assistance of a suite of simple widgets, and discover potential features with several interactions. We further extend the GMM-based exploration scheme to time-varying data sets using an incremental GMM estimation algorithm. The algorithm estimates the GMM for one time step by using itself and the GMM generated from its previous steps. Sequentially applying the incremental algorithm to all time steps in a selected time interval yields a preliminary classification for each time step. In addition, the computed ETFs can be freely adjusted. The adjustments are then automatically propagated to other time steps. In this way, coherent user-guided exploration of a given time interval is achieved. Our GPU implementation demonstrates interactive performance and good scalability. The effectiveness of our approach is verified on several data sets.","tags":["Volume classification","volume rendering","Gaussian mixture model","time-varying data","temporal coherence"],"title":"Efficient Volume Exploration Using the Gaussian Mixture Model.","type":"publication"},{"authors":["Jin Huang","Zherong Pan","Guoning Chen","Wei Chen","Hujun Bao"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"52ef22bd8765c98d718f28341b46a1f5","permalink":"https://zjuvag.github.io/publication/lic/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/lic/","section":"publication","summary":"Image-space line integral convolution (LIC) is a popular scheme for visualizing surface vector fields due to its simplicity and high efficiency. To avoid inconsistencies or color blur during the user interactions, existing approaches employ surface parameterization or 3D volume texture schemes. However, they often require expensive computation or memory cost, and cannot achieve consistent results in terms of both the granularity and color distribution on different scales. This paper introduces a novel image-space surface flow visualization approach that preserves the coherence during user interactions. To make the noise texture under different viewpoints coherent, we propose to precompute a sequence of mipmap noise textures in a coarse-to-fine manner for consistent transition, and map the textures onto each triangle with randomly assigned and constant texture coordinates. Further, a standard image-space LIC is performed to generate the flow texture. The proposed approach is simple and GPU-friendly, and can be easily combined with various texture-based flow visualization techniques. By leveraging viewpoint-dependent backward tracing and mipmap noise phase, our method can be incorporated with the image-based flow visualization (IBFV) technique for coherent visualization of unsteady flows. We demonstrate consistent and highly efficient flow visualization on a variety of data sets.","tags":["Flow visualization","mipmap","LIC","IBFV","surface flows","unsteady flows"],"title":"Image-Space Texture-Based Output-Coherent Surface Flow Visualization.","type":"publication"},{"authors":["Ming-Yuen Chan","Yingcai Wu","JWai-Ho Mak","Wei Chen","Huamin Qu"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3919ad17695801d5afad44855558de97","permalink":"https://zjuvag.github.io/publication/perception-based/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/perception-based/","section":"publication","summary":"The semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume. However, obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters. Furthermore, the visual quality of layers also depends on various perceptual factors. In this paper, we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images. We introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images. By optimizing rendering parameters within an adaptive and intuitive user interaction process, the quality of the images is enhanced such that specific user requirements can be met. Experimental results on various datasets demonstrate the effectiveness and robustness of our method.","tags":["Direct volume rendering","image enhancement","layer perception"],"title":"Perception-Based Transparency Optimization for Direct Volume Rendering.","type":"publication"},{"authors":["Conglei Shi","Weiwei Cui","Shixia Liu","Panpan Xu","Wei Chen","Huamin Qu"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9042ff6c996bb857b8cdc4b7688ef3ec","permalink":"https://zjuvag.github.io/publication/rankexplorer/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publication/rankexplorer/","section":"publication","summary":"For many applications involving time series data, people are often interested in the changes of item values over time as well as their ranking changes. For example, people search many words via search engines like Google and Bing every day. Analysts are interested in both the absolute searching number for each word as well as their relative rankings. Both sets of statistics may change over time. For very large time series data with thousands of items, how to visually present ranking changes is an interesting challenge. In this paper, we propose RankExplorer, a novel visualization method based on ThemeRiver to reveal the ranking changes. Our method consists of four major components: 1) a segmentation method which partitions a large set of time series curves into a manageable number of ranking categories; 2) an extended ThemeRiver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category; 3) a trend curve to show the degree of ranking changes over time; 4) rich user interactions to support interactive exploration of ranking changes. We have applied our method to some real time series data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations.","tags":["Time-series Data","Ranking change","Themeriver","Interaction Techniques"],"title":"RankExplorer: Visualization of Ranking Changes in Large Time Series Data.","type":"publication"},{"authors":["Fangzhou Guo","Tianlong Gu","Wei Chen","FeiranWu","Qi Wang","Lei Shi","Huamin Qu"],"categories":[],"content":"","date":1524210362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524210362,"objectID":"a2206d97807bfb64ba2d3417a239700e","permalink":"https://zjuvag.github.io/publication/tcptree/","publishdate":"2018-04-20T15:46:02+08:00","relpermalink":"/publication/tcptree/","section":"publication","summary":"Discovering the correlations among variables of air quality data is challenging, because the correlation time series are long-lasting, multi-faceted, and information-sparse. In this article, we propose a novel visual representation, called Time-correlation-partitioning (TCP) tree, that compactly characterizes correlations of multiple air quality variables and their evolutions. A TCP tree is generated by partitioning the information-theoretic correlation time series into pieces with respect to the variable hierarchy and temporal variations, and reorganizing these pieces into a hierarchically nested structure. The visual exploration of a TCP tree provides a sparse data traversal of the correlation variations and a situation-aware analysis of correlations among variables. This can help meteorologists understand the correlations among air quality variables better. We demonstrate the efficiency of our approach in a real-world air quality investigation scenario.","tags":["Sensor","Multivariate time-series","Information Theory","Transfer Entropy"],"title":"Visual Exploration of Air Quality Data with A Time-Correlation Partitioning Tree Based on Information Theory.","type":"publication"},{"authors":["Wei Chen","Jing Xia","Xumeng Wang","Yi Wang","Jun Chen","Tianlong Gu"],"categories":[],"content":"","date":1519890362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519890362,"objectID":"50a7821e2c5a615a6a1f400bf57af1b2","permalink":"https://zjuvag.github.io/publication/relationlines/","publishdate":"2018-03-01T15:46:02+08:00","relpermalink":"/publication/relationlines/","section":"publication","summary":"The increased accessibility of urban sensor data and the popularity of social network applications is enabling the discovery of crowd mobility and personal communication patterns. However, studying the egocentric relationships of an individual (i.e., the egocentric relations) can be very challenging because available data may refer to direct contacts, such as phone calls between individuals, or indirect contacts, such as paired location presence. In this paper, we develop methods to integrate three facets extracted from heterogeneous urban data (timelines, calls and locations) through a progressive visual reasoning and inspection scheme. Our approach uses a detect-and-filter scheme, such that, prior to visual refinement and analysis, a coarse detection is performed to extract the target individual and construct the timeline of the target. It then detects spatio-temporal co-occurrences or call-based contacts to develop the egocentric network of the individual. The filtering stage is enhanced with a line-based visual reasoning interface that facilitates flexible and comprehensive investigation of egocentric relationships and connections in terms of time, space and social networks. The integrated system, RelationLines, is demonstrated using a dataset that contains taxi GPS data, cell-base mobility data, mobile calling data, microblog data and POI data of a city with millions of citizens. We examine the effectiveness and efficiency of our system by three case studies and user review.","tags":["Location-based","Egocentric Relations","Visual Reasoning","Heterogeneous Urban Data","Timeline"],"title":"Relationlines: Visual Reasoning of Egocentric Relations from Heterogeneous Urban Data","type":"publication"},{"authors":["Xumeng Wang","Jia-Kai Chou","Wei Chen","Huihua Guan","Wenlong Chen","Tianyi Lao Kwan-Liu Ma"],"categories":[],"content":"","date":1503992762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503992762,"objectID":"623ae77fb54e58d81edff10ba4f12f96","permalink":"https://zjuvag.github.io/publication/utilityaware/","publishdate":"2017-08-29T15:46:02+08:00","relpermalink":"/publication/utilityaware/","section":"publication","summary":"Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufcient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.","tags":["Privacy Preservating Visualization","Utility Aware Anonymization","Syntactic Anonymity","Differential Privacy"],"title":"A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data.","type":"publication"},{"authors":["Jing Xia","Wei Chen","Yumeng Hou","Xinxin Huang","Wanqi Hu","David S.Ebert"],"categories":[],"content":"","date":1490227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490227200,"objectID":"9bf1a5a180a958d8cf237f5083a8846a","permalink":"https://zjuvag.github.io/publication/dimscanner/","publishdate":"2017-03-23T00:00:00Z","relpermalink":"/publication/dimscanner/","section":"publication","summary":"Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with a data structuring and exploration scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assistance widgets to interactively detect the relations of multi-dimensional data.","tags":["Correlation","Exploration"],"title":"DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection.","type":"publication"},{"authors":["Jing Xia","Yumeng Hou","Victor Chen","Cheryl Qian","David S. Ebert","Wei Chen"],"categories":[],"content":"","date":1489536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489536000,"objectID":"16ca73328ffc1e52dad420621eac08d8","permalink":"https://zjuvag.github.io/publication/visualizing-rank-time-series-of-wikipedia-top-viewed-pages/","publishdate":"2017-03-15T00:00:00Z","relpermalink":"/publication/visualizing-rank-time-series-of-wikipedia-top-viewed-pages/","section":"publication","summary":"Visual clutter is a common challenge when visualizing large rank time series data. WikiTopReader, a reader of Wikipedia page rank, lets users explore connections among top-viewed pages by connecting page-rank behaviors with page-link relations. Such a combination enhances the unweighted Wikipedia page-link network and focuses attention on the page of interest. A set of user evaluations shows that the system effectively represents evolving ranking patterns and page-wise correlation.","tags":["computer graphics","rank time series","page view","page link","visualization"],"title":"Visualizing Rank Time Series of Wikipedia Top Viewed Pages.","type":"publication"},{"authors":["Wei Chen","Tianyi Lao","Jing Xia","Xinxin Huang","Biao Zhu","Wanqi Hu","Huihua Guan"],"categories":[],"content":"","date":1474934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474934400,"objectID":"3404717b4b4b3be26c3c43c19d742106","permalink":"https://zjuvag.github.io/publication/gameflow/","publishdate":"2016-09-27T00:00:00Z","relpermalink":"/publication/gameflow/","section":"publication","summary":"Although basketball games have received broad attention, the forms of game reports and webcast are purely content-based cross-media: texts, videos, snapshots, and performance figures. Analytical narrations of games that seek to compose a complete game from heterogeneous datasets are challenging for general media producers because such a composition is time-consuming and heavily depends on domain experts. In particular, an appropriate analytical commentary of basketball games requires two factors, namely, rich context and domain knowledge, which includes game events, player locations, player profiles, and team profiles, among others. This type of analytical commentary elicits a timely and effective basketball game data visualization made up of different sources of media. Existing visualizations of basketball games mainly profile a particular aspect of the game. Therefore, this paper presents an expressive visualization scheme that comprehensively illustrates NBA games with three levels of details: a season level, a game level, and a session level. We reorganize a basketball game as a sequence of sessions to depict the game states and heated confrontations. We design and implement a live system that integrates multimedia NBA datasets: play-by-play text data, box score data, game video data, and action area data. We demonstrate the effectiveness of this scheme with case studies and user feedbacks.","tags":["Game","Video","Trajectory"],"title":"GameFlow: Narrative Visualization of NBA Basketball Games","type":"publication"},{"authors":["Shamal Al-Dohuki","Yingyu Wu","Farah Kamw","Jing Yang","Xin Li","Ye Zhao","Xinyue Ye","Wei Chen","Chao Ma","Fei Wang"],"categories":[],"content":"","date":1472083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472083200,"objectID":"5c2501a065db0b3249bd9b310b909571","permalink":"https://zjuvag.github.io/publication/semantictraj/","publishdate":"2016-08-25T00:00:00Z","relpermalink":"/publication/semantictraj/","section":"publication","summary":"Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as “What were the taxi trips starting from Main Street and ending at Wall Street in the morning?” or “Where are the taxis arriving at the Art Museum at noon typically coming from?”, tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as “Main Street”, “Wall Street”, and “Art Museum”. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.","tags":["Semantic","Taxi","Trajectory","Question","Urban","Transportation","Search"],"title":"SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories","type":"publication"},{"authors":["Chris Muelder","Biao Zhu","Wei Chen","Hongxin Zhang","Kwan-Liu Ma"],"categories":[],"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"174e38301098ac9625873ddf04c16620","permalink":"https://zjuvag.github.io/publication/visual-analysis-of-cloud-computing-performance-using-behavioral-lines/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/visual-analysis-of-cloud-computing-performance-using-behavioral-lines/","section":"publication","summary":"Cloud computing is an essential technology to Big Data analytics and services. A cloud computing system is often comprised of a large number of parallel computing and storage devices. Monitoring the usage and performance of such a system is important for efficient operations, maintenance, and security. Tracing every application on a large cloud system is untenable due to scale and privacy issues. But profile data can be collected relatively efficiently by regularly sampling the state of the system, including properties such as CPU load, memory usage, network usage, and others, creating a set of multivariate time series for each system. Adequate tools for studying such large-scale, multidimensional data are lacking. In this paper, we present a visual based analysis approach to understanding and analyzing the performance and behavior of cloud computing systems. Our design is based on similarity measures and a layout method to portray the behavior of each compute node over time. When visualizing a large number of behavioral lines together, distinct patterns often appear suggesting particular types of performance bottleneck. The resulting system provides multiple linked views, which allow the user to interactively explore the data by examining the data or a selected subset at different levels of detail. Our case studies, which use datasets collected from two different cloud systems, show that this visual based approach is effective in identifying trends and anomalies of the systems.","tags":["Cloud computing","multidimensional data","performance visualization","visual analytics"],"title":"Visual Analysis of Cloud Computing Performance Using Behavioral Lines.","type":"publication"},{"authors":["Xinhu Zheng","Wei Chen","Pu Wang","Dayong Shen","Songhang Chen","Xiao Wang","Qingpeng Zhang","Liuqin Yang"],"categories":[],"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"6ddc0dda4162458e8cab3d1eefda16fe","permalink":"https://zjuvag.github.io/publication/big-data-in-social-transportation/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/big-data-in-social-transportation/","section":"publication","summary":"Big data for social transportation brings us unprecedented opportunities for resolving transportation problems for which traditional approaches are not competent and for building the next-generation intelligent transportation systems. Although social data have been applied for transportation analysis, there are still many challenges. First, social data evolve with time and contain abundant information, posing a crucial need for data collection and cleaning. Meanwhile, each type of data has specific advantages and limitations for social transportation, and one data type alone is not capable of describing the overall state of a transportation system. Systematic data fusing approaches or frameworks for combining social signal data with different features, structures, resolutions, and precision are needed. Second, data processing and mining techniques, such as natural language processing and analysis of streaming data, require further revolutions in effective utilization of real-time traffic information. Third, social data are connected to cyber and physical spaces. To address practical problems in social transportation, a suite of schemes are demanded for realizing big data in social transportation systems, such as crowdsourcing, visual analysis, and task-based services. In this paper, we overview data sources, analytical approaches, and application systems for social transportation, and we also suggest a few future research directions for this new social transportation field.","tags":["Big data","social transportation","intelligent transportation system","data analytics","crowdsourcing"],"title":"Big Data in Social Transportation.","type":"publication"},{"authors":["Hongsen Liao","Li Chen","Yingcai Wu","Yunhai Wang","Huizhang","Wei Chen"],"categories":[],"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"434cc3f7913bce56c108c870165b8240","permalink":"https://zjuvag.github.io/publication/a-visual-voting-framework-for-weather-forecast-calibration/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/publication/a-visual-voting-framework-for-weather-forecast-calibration/","section":"publication","summary":"Numerical weather predictions have been widely used for weather forecasting. Many large meteorological centers are producing highly accurate ensemble forecasts routinely to provide effective weather forecast services. However, biases frequently exist in forecast products because of various reasons, such as the imperfection of the weather forecast models. Failure to identify and neutralize the biases would result in unreliable forecast products that might mislead analysts; consequently, unreliable weather predictions are produced. The analog method has been commonly used to overcome the biases. Nevertheless, this method has some serious limitations including the difficulties in finding effective similar past forecasts, the large search space for proper parameters and the lack of support for interactive, real-time analysis. In this study, we develop a visual analytics system based on a novel voting framework to circumvent the problems. The framework adopts the idea of majority voting to combine judiciously the different variants of analog methods towards effective retrieval of the proper analogs for calibration. The system seamlessly integrates the analog methods into an interactive visualization pipeline with a set of coordinated views that characterizes the different methods. Instant visual hints are provided in the views to guide users in finding and refining analogs. We have worked closely with the domain experts in the meteorological research to develop the system. The effectiveness of the system is demonstrated using two case studies. An informal evaluation with the experts proves the usability and usefulness of the system.","tags":["Weather forecast","analog method","calibration","majority voting","visual analytics"],"title":"A Visual Voting Framework for Weather Forecast Calibration.","type":"publication"},{"authors":["Fei Wang","Wei Chen","Feiran Wu","Ye Zhao","Han Hong","Tianyu Gu","Long Wang","Ronghua Liang","Hujun Bao"],"categories":[],"content":"","date":1426464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426464000,"objectID":"3eeca3aef5fea1af1e76d53e65ec0228","permalink":"https://zjuvag.github.io/publication/visual_reasoning/","publishdate":"2015-03-16T00:00:00Z","relpermalink":"/publication/visual_reasoning/","section":"publication","summary":"Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset ( 30GB) show that our system performs well for on-demand transport assessment and reasoning.","tags":["Roads","Trajectory","Global Positioning System","Topology","Visualization","Indexes"],"title":"A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Road.","type":"publication"},{"authors":["Wenchao Wu","Yixian Zheng","Huamin Qu","Wei Chen","Eduard Groeller","Lionei Ni"],"categories":[],"content":"","date":1426464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426464000,"objectID":"bc94c2ebd455d732fcb4d5497ed5228f","permalink":"https://zjuvag.github.io/publication/boundaryseer/","publishdate":"2015-03-16T00:00:00Z","relpermalink":"/publication/boundaryseer/","section":"publication","summary":"Boundary changes exist ubiquitously in our daily life. From the Antarctic ozone hole to the land desertification, and from the territory of a country to the area within one-hour reach from a downtown location, boundaries change over time. With a large number of time-varying boundaries recorded, people often need to analyze the changes, detect their similarities or differences, and find out spatial and temporal patterns of the evolution for various applications. In this paper, we present a comprehensive visual analytics system, BoundarySeer, to help users gain insight into the changes of boundaries. Our system consists of four major viewers: 1) a global viewer to show boundary groups based on their similarity and the distribution of boundary attributes such as smoothness and perimeter; 2) a region viewer to display the regions encircled by the boundaries and how they are affected by boundary changes; 3) a trend viewer to reveal the temporal patterns in the boundary evolution and potential spatio-temporal correlations; 4) a directional change viewer to encode movements of boundary segments in different directions. Quantitative analyses of boundaries (e.g., similarity measurement and adaptive clustering) and intuitive visualizations (e.g., density map and ThemeRiver) are integrated into these viewers, which enable users to explore boundary changes from different aspects and at different scales. Case studies with two real-world datasets have been carried out to demonstrate the effectiveness of our system.","tags":["Data visualization","Stability analysis","Market research","Visualization","Power system stability","Heating"],"title":"BoundarySeer: Visual Analysis of 2D Boundary Changes.","type":"publication"},{"authors":["Cong Xie","Wei Chen","Xinxin Hunag","Yueqi Hu","Scott Barlowe","Jing Yang"],"categories":[],"content":"","date":1415232000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415232000,"objectID":"5617d5a7034bd4afd614c587bf404d1e","permalink":"https://zjuvag.github.io/publication/vaet/","publishdate":"2014-11-06T00:00:00Z","relpermalink":"/publication/vaet/","section":"publication","summary":"Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.","tags":["Decision trees","Feature extraction","Data visualization","Time series analysis","Visual analytics","Probabilistic logic","Time series analysis"],"title":"VAET: A Visual Analytics Approach for E-transactions Time-series.","type":"publication"},{"authors":["Rui Wang","Hujun Bao","Karla Bala","Xianjin Yang","Yazhen Yuan","Wei Chen"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"eb7297a6f7514b601ca39c21673e128f","permalink":"https://zjuvag.github.io/publication/automatic_shader/","publishdate":"2014-11-01T00:00:00Z","relpermalink":"/publication/automatic_shader/","section":"publication","summary":"In this paper, we present a new automatic shader simplification method using surface signal approximation. We regard the entire multi-stage rendering pipeline as a process that generates signals on surfaces, and we formulate the simplification of the fragment shader as a global simplification problem across multi-shader stages. Three new shader simplification rules are proposed to solve the problem. First, the code transformation rule transforms fragment shader code to other shader stages in order to redistribute computations on pixels up to the level of geometry primitives. Second, the surface-wise approximation rule uses high-order polynomial basis functions on surfaces to approximate pixel-wise computations in the fragment shader. These approximations are pre-cached and simplify computations at runtime. Third, the surface subdivision rule tessellates surfaces into smaller patches. It combines with the previous two rules to approximate pixel-wise signals at different levels of tessellations with different computation times and visual errors. To evaluate simplified shaders using these simplification rules, we introduce a new cost model that includes the visual quality, rendering time and memory consumption. With these simplification rules and the cost model, we present an integrated shader simplification algorithm that is capable of automatically generating variants of simplified shaders and selecting a sequence of preferable shaders. Results show that the sequence of selected simplified shaders balance performance, accuracy and memory consumption well.","tags":["TGPU shader","real-time rendering","shader simplification","surface signal approximation"],"title":"Automatic Shader Simplification using Surface Signal Approximation.","type":"publication"},{"authors":["Long Zhang","Ying He","Jiazhi Xia","Xuexiang Xie","Wei Chen"],"categories":[],"content":"","date":1285200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285200000,"objectID":"1560ac583a04fd6282e9e6225fadcc89","permalink":"https://zjuvag.github.io/publication/real-timeshape/","publishdate":"2010-09-23T00:00:00Z","relpermalink":"/publication/real-timeshape/","section":"publication","summary":"This paper presents a novel object-space line drawing algorithm that can depict shapes with view-dependent feature lines in real time. Strongly inspired by the Laplacian-of-Gaussian (LoG) edge detector in image processing, we define Laplacian lines as the zero-crossing points of the Laplacian of the surface illumination. Compared to other view-dependent feature lines, Laplacian lines are computationally efficient because most expensive computations can be preprocessed. We further extend Laplacian lines to volumetric data and develop the algorithm to compute volumetric Laplacian lines without isosurface extraction. We apply the proposed Laplacian lines to a wide range of real-world models and demonstrate that Laplacian lines are more efficient than the existing computer generated feature lines, and can be used in interactive graphics applications.","tags":["Laplacian","Feature extraction"],"title":"Real-Time Shape Illustration Using Laplacian Lines","type":"publication"},{"authors":["Wei Chen","Zi’ang Ding","Song Zhang","Anna MacKay-Brandt","Stephen Correia","Huamin Qu","John Allen Crow","David F. Tate","Zhicheng Yan","Qunsheng Peng"],"categories":[],"content":"","date":1256256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1256256000,"objectID":"9e7b36af24bf5906177a4fb113c30b32","permalink":"https://zjuvag.github.io/publication/dti/","publishdate":"2009-10-23T00:00:00Z","relpermalink":"/publication/dti/","section":"publication","summary":"Visual exploration is essential to the visualization and analysis of densely sampled 3D DTI fibers in biological speciments, due to the high geometric, spatial, and anatomical complexity of fiber tracts. Previous methods for DTI fiber visualization use zooming, color-mapping, selection, and abstraction to deliver the characteristics of the fibers. However, these schemes mainly focus on the optimization of visualization in the 3D space where cluttering and occlusion make grasping even a few thousand fibers difficult. This paper introduces a novel interaction method that augments the 3D visualization with a 2D representation containing a low-dimensional embedding of the DTI fibers. This embedding preserves the relationship between the fibers and removes the visual clutter that is inherent in 3D renderings of the fibers. This new interface allows the user to manipulate the DTI fibers as both 3D curves and 2D embedded points and easily compare or validate his or her results in both domains. The implementation of the framework is GPU based to achieve real-time interaction. The framework was applied to several tasks, and the results show that our method reduces the user's workload in recognizing 3D DTI fibers and permits quick and accurate DTI fiber selection.","tags":["Diffusion Tensor Imaging","Fibers","Fiber Clustering","Visualization Interface"],"title":"A Novel Interface for Interactive Exploration of DTI Fibers.","type":"publication"},{"authors":["Wei Chen","Zhicheng Yan","Song Zhang","John Allen Crow","David S. Ebert","R. McLaughlin","K. Mullins","R. Cooper","Zi’ang Ding","Jun Liao"],"categories":[],"content":"","date":1246267204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246267204,"objectID":"db8a1fbf9ae162d1326c5c898fdd2595","permalink":"https://zjuvag.github.io/publication/vimdti/","publishdate":"2009-06-29T17:20:04+08:00","relpermalink":"/publication/vimdti/","section":"publication","summary":"Medical illustration has demonstrated its effectiveness to depict salient anatomical features while hiding the irrelevant details. Current solutions are ineffective for visualizing fibrous structures such as muscle, because typical datasets (CT or MRI) do not contain directional details. In this paper, we introduce a new muscle illustration approach that leverages diffusion tensor imaging (DTI) data and example-based texture synthesis techniques. Beginning with a volumetric diffusion tensor image, we reformulate it into a scalar field and an auxiliary guidance vector field to represent the structure and orientation of a muscle bundle. A muscle mask derived from the input diffusion tensor image is used to classify the muscle structure. The guidance vector field is further refined to remove noise and clarify structure. To simulate the internal appearance of the muscle, we propose a new two-dimensional examplebased solid texture synthesis algorithm that builds a solid texture constrained by the guidance vector field. Illustrating the constructed scalar field and solid texture efficiently highlights the global appearance of the muscle as well as the local shape and structure of the muscle fibers in an illustrative fashion. We have applied the proposed approach to five example datasets (four pig hearts and a pig leg), demonstrating plausible illustration and expressiveness.","tags":["Illustrative Visualization","Diffusion Tensor Image","Muscle","Solid Texture Synthesis"],"title":"Volume Illustration of Muscle from Diffusion Tensor Images.","type":"publication"},{"authors":["Fei Wang","Wei Chen","Ye Zhao","Tianyu Gu","Siyuan Gao","Hujun Bao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"219925be602f153b2b693d53b019f715","permalink":"https://zjuvag.github.io/publication/populationmobilitypatterns/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/populationmobilitypatterns/","section":"publication","summary":"Thanks to the ubiquitous cell phone use, we have never been so close to uncover population mobility patterns in urban area. While some researches utilize cellphone call records to mine population patterns, few works aim to depict population movement in adaptively spatial and temporal representations, i.e., from a community, a district in the city over an hour, a day to a week. In this paper, we construct a system which deciphers, transforms, queries, and visualizes the records from the millions of users in a city. In particular, we design a data structure, namely MobiHash, which collects phone call records over base stations and indexes them by utilizing a Voronoi division of the urban space. MobiHash supports responsive data queries so that users can interactively retrieve trajectories reflecting population flows in areas of interest. Moreover, population movement is represented as vector fields to reduce visual clutter and occlusions. Because of sparse moving points, a novel radiation model is proposed to interpolate population passing zones. Case studies and experts' feedback validate the utility and efficiency by comparing population moving patterns in different times by using our system.","tags":["Population Mobility Pattern","Visual Query","Flow Visualization","Cell Phone Data"],"title":"Adaptively Exploring Population Mobility Patterns in Flow Visualization","type":"publication"},{"authors":["Hongsen Liao","Yingcai Wu","Li Chen","Wei Chen"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"158ec8ea93623842e860a8738bfe3dac","permalink":"https://zjuvag.github.io/publication/cluster-based/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/cluster-based/","section":"publication","summary":"The use of scatterplots is an important method for multivariate data visualization. The point distribution on the scatterplot, along with variable values represented by each point, can help analyze underlying patterns in data. However, determining the multivariate data variation on a scatterplot generated using projection methods, such as multidimensional scaling, is difficult. Furthermore, the point distribution becomes unclear when the data scale is large and clutter problems occur. These conditions can significantly decrease the usability of scatterplots on multivariate data analysis. In this study, we present a cluster-based visual abstraction method to enhance the visualization of multivariate scatterplots. Our method leverages an adapted multilabel clustering method to provide abstractions of high quality for scatterplots. An image-based method is used to deal with large scale data problem. Furthermore, a suite of glyphs is designed to visualize the data at different levels of detail and support data exploration. The view coordination between the glyph-based visualization and the table lens can effectively enhance the multivariate data analysis. Through numerical evaluations for data abstraction quality, case studies and a user study, we demonstrate the effectiveness and usability of the proposed techniques for multivariate data analysis on scatterplots.","tags":["Data abstraction","scatterplot","glyph visualization","multilabel optimization"],"title":"Cluster-based Visual Abstraction for Multivariate Scatterplots","type":"publication"},{"authors":["Jiazhi Xia","Fenjin Ye","Wei Chen","Yusi Wang","Weifeng Chen","Yuxin Ma","Anthony K.H. Tung"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d078ad8ab9a7a02bd5943285ca40e364","permalink":"https://zjuvag.github.io/publication/ldsscanner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ldsscanner/","section":"publication","summary":"Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the x axis and y axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (x axis) and the variation of LTS in structures (the combination of x axis and y axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.","tags":["High-dimensional data","low-dimensional structure","subspace","manifold","visual exploration"],"title":"LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets","type":"publication"},{"authors":["Ross Maciejewski","Insoo Wu","Wei Chen","David S. Ebert"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db155ef28a4d03ae6e582833173fd023","permalink":"https://zjuvag.github.io/publication/structuringfeaturespace/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/structuringfeaturespace/","section":"publication","summary":"The use of multi-dimensional transfer functions for direct volume rendering has been shown to be an effective means of extracting materials and their boundaries for both scalar and multivariate data. The most common multi-dimensional transfer function consists of a two-dimensional (2D) histogram with axes representing a subset of the feature space (e.g., value vs. value gradient magnitude), with each entry in the 2D histogram being the number of voxels at a given feature space pair. Users then assign color and opacity to the voxel distributions within the given feature space through the use of interactive widgets (e.g., box, circular, triangular selection). Unfortunately, such tools lead users through a trial-and-error approach as they assess which data values within the feature space map to a given area of interest within the volumetric space. In this work, we propose the addition of non-parametric clustering within the transfer function feature space in order to extract patterns and guide transfer function generation. We apply a non-parametric kernel density estimation to group voxels of similar features within the 2D histogram. These groups are then binned and colored based on their estimated density, and the user may interactively grow and shrink the binned regions to explore feature boundaries and extract regions of interest. We also extend this scheme to temporal volumetric data in which time steps of 2D histograms are composited into a histogram volume. A three-dimensional (3D) density estimation is then applied, and users can explore regions within the feature space across time without adjusting the transfer function at each time step. Our work enables users to effectively explore the structures found within a feature space of the volume and provide a context in which the user can understand how these structures relate to their volumetric data. We provide tools for enhanced exploration and manipulation of the transfer function, and we show that the initial transfer function generation serves as a reasonable base for volumetric rendering, reducing the trial-and-error overhead typically found in transfer function design.","tags":["Volume rendering","kernel density estimation","transfer function design","temporal volume rendering"],"title":"Structuring Feature Space: A Non-Parametric Method for Volumetric Transfer Function Generation.","type":"publication"},{"authors":["Gennady Andrienko","Natalia Andrienko","Wei Chen","Ross Maciejewski","Ye Zhao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"97c877c391f50c41f9513ffa95f5a613","permalink":"https://zjuvag.github.io/publication/mobilityandtransportation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mobilityandtransportation/","section":"publication","summary":"Many cities and countries are now striving to create intelligent transportation systems that utilize the current abundance of multisource and multiform data related to the functionality and the use of transportation infrastructure to better support human mobility, interests, and lifestyles. Such intelligent transportation systems aim to provide novel services that can enable transportation consumers and managers to be better informed and make safer and more efficient use of the infrastructure. However, the transportation domain is characterized by both complex data and complex problems, which calls for visual analytics approaches. The science of visual analytics is continuing to develop principles, methods, and tools to enable synergistic work between humans and computers through interactive visual interfaces. Such interfaces support the unique capabilities of humans (such as the flexible application of prior knowledge and experiences, creative thinking, and insight) and couple these abilities with machines' computational strengths, enabling the generation of new knowledge from large and complex data. In this paper, we describe recent developments in visual analytics that are related to the study of movement and transportation systems and discuss how visual analytics can enable and improve the intelligent transportation systems of the future. We provide a survey of literature from the visual analytics domain and organize the survey with respect to the different types of transportation data, movement and its relationship to infrastructure and behavior, and modeling and planning. We conclude with lessons learned and future directions, including social transportation, recommender systems, and policy implications.","tags":["Data visualization","graphical user interfaces","interactive systems"],"title":"Visual Analytics of Mobility and Transportation: State of the Art and Further Research Directions.","type":"publication"}]