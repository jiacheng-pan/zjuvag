[{"authors":["admin"],"categories":null,"content":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009). The team works extensively with universities and research institutions at home and abroad, including Purdue University, Hong Kong University of Science and Technology, University of California, Davis, North Carolina, Mississippi State University, Bosch North American Institute, Microsoft Research Asia, National Meteorological Administration. , Ali Group and so on. The world\u0026rsquo;s first non-photorealistic 3D GPS navigation system, developed by our group\u0026rsquo;s five members at the Bosch North American Institute, has entered the global automotive market. Our group has developed a data visualization component library, DataV, together with the data product division of Ali Group. It has now been widely used within the Ali Group. The global scale 3D numerical atmospheric visual analysis system developed by our group has been well received by the National Meteorological Administration.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zjuvag.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The Visual Analytics Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大陈为).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 20 papers on IEEE VIS and IEEE/ACM articles, including the first three papers published by the Chinese mainland at this conference (2004, 2009).","tags":null,"title":"ZJU VAG","type":"authors"},{"authors":["bingrulin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3a2ae9400bfd4148d0695ace0ec9b9d0","permalink":"https://zjuvag.github.io/authors/bingrulin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bingrulin/","section":"authors","summary":"","tags":null,"title":"Bingru Lin","type":"authors"},{"authors":["dongminghan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"09d65d06c3a9fe08d385587c9e6e87c5","permalink":"https://zjuvag.github.io/authors/dongminghan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dongminghan/","section":"authors","summary":"","tags":null,"title":"Dongming Han","type":"authors"},{"authors":["fanyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f214ff4a10bf6ecb96d09346184e3ef6","permalink":"https://zjuvag.github.io/authors/fanyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanyan/","section":"authors","summary":"","tags":null,"title":"Fan Yan","type":"authors"},{"authors":["haozhefeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e42d6a3136ba277bd69c8cb130f9e331","permalink":"https://zjuvag.github.io/authors/haozhefeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haozhefeng/","section":"authors","summary":"","tags":null,"title":"Haozhe Feng","type":"authors"},{"authors":["honghuimei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"58c4d52fdd159c66b67b8953d7b59569","permalink":"https://zjuvag.github.io/authors/honghuimei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/honghuimei/","section":"authors","summary":"","tags":null,"title":"Honghui Mei","type":"authors"},{"authors":["jiachengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4fb9f5907ebada51e66a95964ef449db","permalink":"https://zjuvag.github.io/authors/jiachengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiachengpan/","section":"authors","summary":"","tags":null,"title":"Jiacheng Pan","type":"authors"},{"authors":["jianweizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b3c918c34f1f2f1ec813f07c2d25fc85","permalink":"https://zjuvag.github.io/authors/jianweizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianweizhang/","section":"authors","summary":"","tags":null,"title":"Jianwei Zhang","type":"authors"},{"authors":["jiapinlu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1c534ed953a645c73533138708fa334e","permalink":"https://zjuvag.github.io/authors/jiapinlu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiapinlu/","section":"authors","summary":"","tags":null,"title":"Jiapin Lu","type":"authors"},{"authors":["jiehuizhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ecf94669d677925eacdd07fb6e2eecc6","permalink":"https://zjuvag.github.io/authors/jiehuizhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiehuizhou/","section":"authors","summary":"","tags":null,"title":"Jiehui Zhou","type":"authors"},{"authors":["jiewang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"81f2479fbce3bbda2fbae0889ec3bdfc","permalink":"https://zjuvag.github.io/authors/jiewang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiewang/","section":"authors","summary":"","tags":null,"title":"Jie Wang","type":"authors"},{"authors":["jinglixu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4ce8d75fde5d06b78f0badc0477bed6c","permalink":"https://zjuvag.github.io/authors/jinglixu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jinglixu/","section":"authors","summary":"","tags":null,"title":"Jingli Xu","type":"authors"},{"authors":["junhualu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a064b7e3b2d2ced15b4acef508a1049e","permalink":"https://zjuvag.github.io/authors/junhualu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junhualu/","section":"authors","summary":"","tags":null,"title":"Junhua Lu","type":"authors"},{"authors":["kejieyu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"618564ca10a1f5ed7a789b6f96efb2fa","permalink":"https://zjuvag.github.io/authors/kejieyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kejieyu/","section":"authors","summary":"","tags":null,"title":"Kejie Yu","type":"authors"},{"authors":["linhaomeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"47178a400c59f544be2df10ed1936483","permalink":"https://zjuvag.github.io/authors/linhaomeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/linhaomeng/","section":"authors","summary":"","tags":null,"title":"Linhao Meng","type":"authors"},{"authors":["lonapalawongsupaporn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6430ec5fa919d99a92b083d106ec81db","permalink":"https://zjuvag.github.io/authors/lonapalawongsupaporn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lonapalawongsupaporn/","section":"authors","summary":"","tags":null,"title":"Lonapalawong Supaporn","type":"authors"},{"authors":["minfengzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d2e2d2887bb13006210f344c3d2b4856","permalink":"https://zjuvag.github.io/authors/minfengzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minfengzhu/","section":"authors","summary":"","tags":null,"title":"Minfeng Zhu","type":"authors"},{"authors":["noptanitchotisarn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6a7eec2abe6ed13991d82a70ef34b879","permalink":"https://zjuvag.github.io/authors/noptanitchotisarn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/noptanitchotisarn/","section":"authors","summary":"","tags":null,"title":"Noptanit Chotisarn","type":"authors"},{"authors":["rushengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"6d25cf561fd51b42f5cbd8309adeaa7e","permalink":"https://zjuvag.github.io/authors/rushengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rushengpan/","section":"authors","summary":"","tags":null,"title":"Rusheng Pan","type":"authors"},{"authors":["shengjiegao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"67a79c2b02345bc1e435708934246d65","permalink":"https://zjuvag.github.io/authors/shengjiegao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shengjiegao/","section":"authors","summary":"","tags":null,"title":"Shengjie Gao","type":"authors"},{"authors":["shuyuezhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a07ca3bd20c7f1c5f1254e02b72f184e","permalink":"https://zjuvag.github.io/authors/shuyuezhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuyuezhou/","section":"authors","summary":"","tags":null,"title":"Shuyue Zhou","type":"authors"},{"authors":["siweitan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f29271012bad57d28ce97df779d9648b","permalink":"https://zjuvag.github.io/authors/siweitan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/siweitan/","section":"authors","summary":"","tags":null,"title":"Siwei Tan","type":"authors"},{"authors":["tianyezhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e9e1cdffe6711fa9a43b0864dec396c6","permalink":"https://zjuvag.github.io/authors/tianyezhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyezhang/","section":"authors","summary":"","tags":null,"title":"Tianye Zhang","type":"authors"},{"authors":["weichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"611789f74777bc3ea79ff9520e093a52","permalink":"https://zjuvag.github.io/authors/weichen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weichen/","section":"authors","summary":"","tags":null,"title":"Wei Chen","type":"authors"},{"authors":["weixiaxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f5636b7d8935f736a3e99f5992265e74","permalink":"https://zjuvag.github.io/authors/weixiaxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weixiaxu/","section":"authors","summary":"","tags":null,"title":"Weixia Xu","type":"authors"},{"authors":["wenjielu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"fff1a80c32ac66eee12b7369714a3555","permalink":"https://zjuvag.github.io/authors/wenjielu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenjielu/","section":"authors","summary":"","tags":null,"title":"Wenjie Lu","type":"authors"},{"authors":["xiaodongzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dbc5ba7e8a284577aa94508220ccc51a","permalink":"https://zjuvag.github.io/authors/xiaodongzhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaodongzhao/","section":"authors","summary":"","tags":null,"title":"Xiaodong Zhao","type":"authors"},{"authors":["xvmengwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"91c1e987906416f2f88af79e18a1f92b","permalink":"https://zjuvag.github.io/authors/xvmengwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xvmengwang/","section":"authors","summary":"","tags":null,"title":"Xvmeng Wang","type":"authors"},{"authors":["yankongzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5b54040d35879bf008e0ce0d58ce8582","permalink":"https://zjuvag.github.io/authors/yankongzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yankongzhang/","section":"authors","summary":"","tags":null,"title":"Yankong Zhang","type":"authors"},{"authors":["yatingwei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7f6fca97fe9dc0fb109be329aebe5385","permalink":"https://zjuvag.github.io/authors/yatingwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yatingwei/","section":"authors","summary":"","tags":null,"title":"Yating Wei","type":"authors"},{"authors":["yichaowang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"97c3335e53dd5045cf8375077f3441ff","permalink":"https://zjuvag.github.io/authors/yichaowang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yichaowang/","section":"authors","summary":"","tags":null,"title":"Yichao Wang","type":"authors"},{"authors":["yijingliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7110456b92de4eeab4224e4188560b5f","permalink":"https://zjuvag.github.io/authors/yijingliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yijingliu/","section":"authors","summary":"","tags":null,"title":"Yijing Liu","type":"authors"},{"authors":["yingxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a3d45e8ffeb1f08addba4c8c9545f41b","permalink":"https://zjuvag.github.io/authors/yingxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yingxu/","section":"authors","summary":"","tags":null,"title":"Ying Xu","type":"authors"},{"authors":["youchenggong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"88c5cbdae50149d621ac4f6d5b2ad4f6","permalink":"https://zjuvag.github.io/authors/youchenggong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/youchenggong/","section":"authors","summary":"","tags":null,"title":"Youcheng Gong","type":"authors"},{"authors":["yuanzhehu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e5db2f36fde56f28f6f63167d31fb711","permalink":"https://zjuvag.github.io/authors/yuanzhehu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuanzhehu/","section":"authors","summary":"","tags":null,"title":"Yuanzhe Hu","type":"authors"},{"authors":["yuhuigu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"525f85d17df210782f32184018c43cd8","permalink":"https://zjuvag.github.io/authors/yuhuigu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhuigu/","section":"authors","summary":"","tags":null,"title":"Yuhui Gu","type":"authors"},{"authors":["yuxuanhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"359b2cdc705132c678015f91b08c756b","permalink":"https://zjuvag.github.io/authors/yuxuanhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuxuanhou/","section":"authors","summary":"","tags":null,"title":"Yuxuan Hou","type":"authors"},{"authors":["zexianchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"72b1c74afd05b0bbbe06e0b2c86b7c17","permalink":"https://zjuvag.github.io/authors/zexianchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zexianchen/","section":"authors","summary":"","tags":null,"title":"Zexian Chen","type":"authors"},{"authors":["zhaosonghuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3733917af20bc801c57fb5dfabff0a07","permalink":"https://zjuvag.github.io/authors/zhaosonghuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaosonghuang/","section":"authors","summary":"","tags":null,"title":"Zhaosong Huang","type":"authors"},{"authors":["zhezhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"79c59e66f2a141087e56ef47c2636661","permalink":"https://zjuvag.github.io/authors/zhezhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhezhao/","section":"authors","summary":"","tags":null,"title":"Zhe Zhao","type":"authors"},{"authors":["zhiyongwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2c51f279f4b3732c7164c6424abed2bc","permalink":"https://zjuvag.github.io/authors/zhiyongwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiyongwang/","section":"authors","summary":"","tags":null,"title":"Zhiyong Wang","type":"authors"},{"authors":[],"categories":[],"content":"  论文：SEQ2SEQ-VIS: A Visual Debugging Tool forSequence-to-Sequence Models\n 作者：Hendrik Strobelt; Sebastian Gehrmann; Michael Bhrisch; Adam Perer; Hanspeter Pfister; Alexander M. Rush\n  1.简介 本文介绍了一种针对 seq2seq 模型的可视化调试工具 Seq2Seq-Vis，从而更高效地进行分析和调试模型。\n2.背景  Sequence to Sequence：RNN最重要的一个变种，也叫Encoder-Decoder模型。  基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder，encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义。decoder则负责根据语义向量生成指定的序列。\n这个模型不限制输入和输出的序列长度，应用非常广泛。主要包括：机器翻译，自然语言生成、图像描述以及文本摘要等。\n Attention-based Model：指的是神经网络模型增加Attention机制。  具体来说，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。举例：翻译“Tom chase Jerry”中的Jerry时，“Tom”，“chase”和“Jerry”分别拥有不同的attention权重，即这三个词对翻译后的“杰瑞”有不同的影响力。\n 本文模型：  seq2seq 模型通过五个阶段，将源序列翻译为目标序列：（S1）Encoder：将源序列编码为潜在向量（S2）Decoder：将其解码为目标序列（S3）Attention：编码器和解码器之间注意力联系（S4）Prediction：在每个时间步骤中，预测单词概率（S5）beam Search：集束搜索。\n重点讲一下beam Search：\nSeq2seq模型在给定前缀的情况下预测所有下一个单词的概率。虽然人们可以在每个时间步骤中简单地采用最高概率词，但是这种选择可能导致错误的路径。设置Beam Size为k，永远选择所有序列中概率最大的k个。一旦所有K个光束通过生成停止标记而终止，则最终预测是具有最高分数的翻译。\n3.动机 “黑箱问题”\n1）神经网络模型的复杂结构导致不可解释性和不确定性。\n2）决策过程的不透明导致无法追踪错误源头。\n4.本文工作 Seq2Seq-Vis——针对 seq2seq 模型的可视化调试工具，使用户可以可视化模型执行过程中的注意力、单词预测、搜索树、状态轨迹和近邻词列表等，从而更高效地进行分析和调试。\n目标：\n 检查模型决策：允许用户理解、描述并具体化模型的错误。 连接样本和决策：通过关联内部状态与相关的训练样本来描述模型决策。 测试可选决策：可以对模型内部进行操作。  任务：\n 创建模型所有五个阶段的常见可视编码。 通过查询大型训练样例数据库，实现最近邻搜索。 为模型的不同阶段生成合理的替代决策。  5.可视化设计  界面概览   翻译视图（Translation View）  E：编码器以蓝色显示。\nD：解码器以黄色显示。\nAttention Vis：注意力通过加权的二分连接显示。为了减少视觉混乱，对注意力图进行了修剪。\nTopK List：每个时间步的前K个预测。每个词的概率采用条形图编码，黄色高亮显示它最终选择结果。\n 邻域视图（Neighborhood View）  Seq2seq模型在每个阶段产生高维向量，例如编码器状态，解码器状态或上下文状态， 很难直接解释，但我们可以通过查看产生类似向量的样例来估计它们的含义。\nG反映的是预先计算好状态的数据集的最近邻域的状态轨迹（采用t-SNE或MDS投影）。为了便于理解较长轨迹，F视图中每一个小窗口都是G视图中的切分，重点关注每一个词。H反映的是最近邻域列表。\n 交互  model-focused模式：直接修改模型。\na. 切换成更改attention权重模式。b.选择target中待修改权重的词。c.重复点击encoder中的词，为其赋予更大的权重。d.最后确认修改。e.模型被更改，会按照新给定的权重重新计算。\nlanguage-focused模式：\n 更改source和target，触发比较模式。\n 从TopK List中手动选择最佳单词。触发前缀解码，在特定路径P上强制进行集束搜索。\n  6.总结与未来工作 总结：\n 针对机器翻译中最常用的 Seq2Seq 模型，提出Seq2Seq-Vis可视化调试工具，能够深入探索模型的所有阶段。\n 既能可视化模型的决策过程，又允许开发人员直接修改模型。\n  未来工作：\n 改进投影算法。\n 扩展支持的序列类型，包括音频，图像和视频等。\n  ","date":1561804272,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561804272,"objectID":"d108520b40f29dd9ebe1ed47037de2f2","permalink":"https://zjuvag.github.io/post/seq2seq-vis/","publishdate":"2019-06-29T18:31:12+08:00","relpermalink":"/post/seq2seq-vis/","section":"post","summary":"论文：SEQ2SEQ-VIS: A Visual Debugging Tool forSequence-to-Sequence Models\n 作者：Hendrik Strobelt; Sebastian Gehrmann; Michael Bhrisch; Adam Perer; Hanspeter Pfister; Alexander M. Rush\n  1.简介 本文介绍了一种针对 seq2seq 模型的可视化调试工具 Seq2Seq-Vis，从而更高效地进行分析和调试模型。\n2.背景  Sequence to Sequence：RNN最重要的一个变种，也叫Encoder-Decoder模型。  基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder，encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义。decoder则负责根据语义向量生成指定的序列。\n这个模型不限制输入和输出的序列长度，应用非常广泛。主要包括：机器翻译，自然语言生成、图像描述以及文本摘要等。\n Attention-based Model：指的是神经网络模型增加Attention机制。  具体来说，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。举例：翻译“Tom chase Jerry”中的Jerry时，“Tom”，“chase”和“Jerry”分别拥有不同的attention权重，即这三个词对翻译后的“杰瑞”有不同的影响力。\n 本文模型：  seq2seq 模型通过五个阶段，将源序列翻译为目标序列：（S1）Encoder：将源序列编码为潜在向量（S2）Decoder：将其解码为目标序列（S3）Attention：编码器和解码器之间注意力联系（S4）Prediction：在每个时间步骤中，预测单词概率（S5）beam Search：集束搜索。\n重点讲一下beam Search：\nSeq2seq模型在给定前缀的情况下预测所有下一个单词的概率。虽然人们可以在每个时间步骤中简单地采用最高概率词，但是这种选择可能导致错误的路径。设置Beam Size为k，永远选择所有序列中概率最大的k个。一旦所有K个光束通过生成停止标记而终止，则最终预测是具有最高分数的翻译。\n3.动机 “黑箱问题”\n1）神经网络模型的复杂结构导致不可解释性和不确定性。\n2）决策过程的不透明导致无法追踪错误源头。\n4.本文工作 Seq2Seq-Vis——针对 seq2seq 模型的可视化调试工具，使用户可以可视化模型执行过程中的注意力、单词预测、搜索树、状态轨迹和近邻词列表等，从而更高效地进行分析和调试。\n目标：\n 检查模型决策：允许用户理解、描述并具体化模型的错误。 连接样本和决策：通过关联内部状态与相关的训练样本来描述模型决策。 测试可选决策：可以对模型内部进行操作。  任务：","tags":[],"title":"SEQ2SEQ VIS","type":"post"},{"authors":["Minfeng Zhu","Wei Chen","Jiazhi Xia","Yuxin Ma","Yankong Zhang","Yuetong Luo","Zhaosong Huang","Liangjun Liu"],"categories":[],"content":"","date":1552296004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552296004,"objectID":"6de916936e6e1d6592d35c0d0ceb4176","permalink":"https://zjuvag.github.io/publication/location2vec/","publishdate":"2019-03-11T17:20:04+08:00","relpermalink":"/publication/location2vec/","section":"publication","summary":"Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.","tags":["Human mobility","word embedding","urban computing","spatio-temporal data","visual exploration"],"title":"location2vec: a situation-aware representation for visual exploration of urban locations","type":"publication"},{"authors":["Zhaosong Huang","Yafeng Lu","Elizabeth Mack","Wei Chen","and Ross Maciejewski."],"categories":[],"content":"","date":1546415162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546415162,"objectID":"9f1b1bd800361c419810ea517894999f","permalink":"https://zjuvag.github.io/publication/geouncertainty/","publishdate":"2019-01-02T15:46:02+08:00","relpermalink":"/publication/geouncertainty/","section":"publication","summary":"The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.","tags":["geospatial analysis","uncertainty","visualization","choropleth"],"title":"Exploring the Sensitivity of Choropleths under Attribute Uncertainty.","type":"publication"},{"authors":["Wei Chen","Zhaosong Huang","Feiran Wu","Minfeng Zhu","Huihua Guan","and Ross Maciejewski."],"categories":[],"content":"","date":1541144762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541144762,"objectID":"816bde2fec75bdb8e2d20358198738dd","permalink":"https://zjuvag.github.io/publication/vaud/","publishdate":"2018-11-02T15:46:02+08:00","relpermalink":"/publication/vaud/","section":"publication","summary":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.","tags":["Urban data","Visual Analysis","Visual Reasoning","Heterogeneous","Spatio-temporal"],"title":"VAUD: A visual analysis approach for exploring spatio-temporal urban data.","type":"publication"},{"authors":[],"categories":null,"content":"","date":1540431380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540431380,"objectID":"9cd7b7d7d5fac63c93a72566cfe6a215","permalink":"https://zjuvag.github.io/talk/2018vis-xumeng/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-xumeng/","section":"talk","summary":"VAST 2018: GraphProtector: a Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","tags":["VIS2018"],"title":"Xumeng Wang presenting GraphProtector at IEEE VAST 2018","type":"talk"},{"authors":[],"categories":null,"content":"","date":1540352600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540352600,"objectID":"9fd13cd9454b4f0973f1f27c640c29c3","permalink":"https://zjuvag.github.io/talk/2018vis-han-pan/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talk/2018vis-han-pan/","section":"talk","summary":"InfoVis 2018: Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","tags":["VIS2018"],"title":"Dongming Han and Jiacheng Pan presenting Structure-Based Suggestive Exploration at IEEE VAST 2018","type":"talk"},{"authors":["Tianlong Gu","Minfeng Zhu","Wei Chen","Zhaosong Huang","Ross Maciejewski","and Liang Chang."],"categories":[],"content":"","date":1533195962,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533195962,"objectID":"74f997771a1ca6d75fc235b2958d0447","permalink":"https://zjuvag.github.io/publication/amtg/","publishdate":"2018-08-02T15:46:02+08:00","relpermalink":"/publication/amtg/","section":"publication","summary":"Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.","tags":["Timeline","Mobility","Mobility Transition","Mobility Patterns"],"title":"Structuring Mobility Transition With an Adaptive Graph Representation.","type":"publication"},{"authors":["Wei Chen","Zhicheng Yan","Song Zhang","John Allen Crow","David S. Ebert","R. McLaughlin","K. Mullins","R. Cooper","Zi’ang Ding","Jun Liao"],"categories":[],"content":"","date":1246267204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246267204,"objectID":"db8a1fbf9ae162d1326c5c898fdd2595","permalink":"https://zjuvag.github.io/publication/vimdti/","publishdate":"2009-06-29T17:20:04+08:00","relpermalink":"/publication/vimdti/","section":"publication","summary":"Medical illustration has demonstrated its effectiveness to depict salient anatomical features while hiding the irrelevant details. Current solutions are ineffective for visualizing fibrous structures such as muscle, because typical datasets (CT or MRI) do not contain directional details. In this paper, we introduce a new muscle illustration approach that leverages diffusion tensor imaging (DTI) data and example-based texture synthesis techniques. Beginning with a volumetric diffusion tensor image, we reformulate it into a scalar field and an auxiliary guidance vector field to represent the structure and orientation of a muscle bundle. A muscle mask derived from the input diffusion tensor image is used to classify the muscle structure. The guidance vector field is further refined to remove noise and clarify structure. To simulate the internal appearance of the muscle, we propose a new two-dimensional examplebased solid texture synthesis algorithm that builds a solid texture constrained by the guidance vector field. Illustrating the constructed scalar field and solid texture efficiently highlights the global appearance of the muscle as well as the local shape and structure of the muscle fibers in an illustrative fashion. We have applied the proposed approach to five example datasets (four pig hearts and a pig leg), demonstrating plausible illustration and expressiveness.","tags":["Illustrative Visualization","Diffusion Tensor Image","Muscle","Solid Texture Synthesis"],"title":"Volume Illustration of Muscle from Diffusion Tensor Images.","type":"publication"}]